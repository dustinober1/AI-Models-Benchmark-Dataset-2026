---
phase: 02-statistical-analysis-domain-insights
plan: 05
type: execute
wave: 3
depends_on: [02-01, 02-02, 02-03]
files_modified:
  - src/bootstrap.py
  - scripts/10_statistical_tests.py
  - scripts/12_trend_predictions.py
  - data/processed/statistical_tests.parquet
  - data/processed/trend_predictions.parquet
  - reports/statistical_tests_2026-01-18.md
  - reports/trend_predictions_2026-01-18.md
autonomous: true

must_haves:
  truths:
    - "Group comparisons performed with non-parametric tests (Mann-Whitney U, Kruskal-Wallis)"
    - "Bootstrap confidence intervals quantify uncertainty for all estimates"
    - "Null findings reported alongside significant results (publication bias avoidance)"
    - "2027 trend predictions with uncertainty discussion (NARR-09 requirement)"
  artifacts:
    - path: "src/bootstrap.py"
      provides: "Bootstrap confidence interval utilities"
      exports: ["bootstrap_mean_ci", "bootstrap_median_ci", "bootstrap_correlation_ci"]
      min_lines: 150
    - path: "scripts/10_statistical_tests.py"
      provides: "Statistical testing pipeline"
      min_lines: 150
    - path: "scripts/12_trend_predictions.py"
      provides: "Trend prediction pipeline"
      min_lines: 100
    - path: "reports/statistical_tests_2026-01-18.md"
      provides: "Narrative report with significant and null findings"
    - path: "reports/trend_predictions_2026-01-18.md"
      provides: "2027 trend extrapolation with uncertainty discussion"
  key_links:
    - from: "scripts/10_statistical_tests.py"
      to: "data/processed/ai_models_deduped.parquet"
      via: "Input deduplicated dataset"
      pattern: "ai_models_deduped"
    - from: "scripts/12_trend_predictions.py"
      to: "data/processed/ai_models_deduped.parquet"
      via: "Input deduplicated dataset"
      pattern: "ai_models_deduped"
    - from: "src/bootstrap.py"
      to: "scipy.stats.bootstrap"
      via: "Bootstrap CI computation with BCa method"
      pattern: "from scipy\\.stats import bootstrap"
    - from: "scripts/10_statistical_tests.py"
      to: "src/bootstrap.py"
      via: "Import bootstrap utilities"
      pattern: "from src\\.bootstrap import"
---

<objective>
Perform group comparisons with non-parametric tests, quantify uncertainty via bootstrap resampling, and generate 2027 trend predictions with comprehensive uncertainty discussion.

Purpose: Complete STAT-07 (bootstrap), STAT-08 (multiple testing), STAT-09 (uncertainty quantification), STAT-10 (predictive modeling), STAT-11 (null findings), NARR-07 (methodology), and NARR-09 (uncertainty discussion) requirements using robust statistical methods appropriate for non-normal distributions.

Output: Statistical test results with bootstrap CIs, trend predictions with uncertainty bounds, and comprehensive documentation of methodology, significant findings, null findings, and limitations (STAT-07, STAT-08, STAT-09, STAT-10, STAT-11, NARR-07, NARR-09).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-statistical-analysis-domain-insights/02-RESEARCH.md

@data/processed/ai_models_deduped.parquet
@data/processed/pareto_frontier.parquet
@data/processed/provider_clusters.parquet
@src/analyze.py
@src/statistics.py
@.planning/phases/02-statistical-analysis-domain-insights/02-01-SUMMARY.md
@.planning/phases/02-statistical-analysis-domain-insights/02-02-SUMMARY.md
@.planning/phases/02-statistical-analysis-domain-insights/02-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement bootstrap and statistical testing utilities</name>
  <files>src/bootstrap.py</files>
  <action>
Create src/bootstrap.py with script-as-module pattern:

**Functions to implement:**

1. `bootstrap_mean_ci(data: np.ndarray, confidence_level: float = 0.95, n_resamples: int = 9999, random_state: int = 42) -> dict`
   - Use scipy.stats.bootstrap with method='BCa'
   - Resample data as (data,) tuple (required by bootstrap)
   - Define statistic function: lambda x: np.mean(x)
   - Return dict: {mean, ci_low, ci_high, standard_error, n_resamples, method}

2. `bootstrap_median_ci(data: np.ndarray, confidence_level: float = 0.95, n_resamples: int = 9999, random_state: int = 42) -> dict`
   - Same as bootstrap_mean_ci but for median
   - Statistic function: lambda x: np.median(x)
   - Return dict with same structure

3. `bootstrap_correlation_ci(x: np.ndarray, y: np.ndarray, confidence_level: float = 0.95, n_resamples: int = 9999, random_state: int = 42) -> dict`
   - Bootstrap Spearman correlation with CI
   - Statistic function: compute Spearman correlation on resampled data
   - Handle pair-wise resampling (resample indices, apply to both x and y)
   - Return dict: {correlation, ci_low, ci_high, standard_error}

4. `bootstrap_group_difference_ci(group1: np.ndarray, group2: np.ndarray, confidence_level: float = 0.95, n_resamples: int = 9999, random_state: int = 42) -> dict`
   - Bootstrap difference in means between two groups
   - Statistic function: lambda data: np.mean(data[0]) - np.mean(data[1])
   - Data format: (group1, group2) tuple
   - Return dict: {mean_difference, ci_low, ci_high, group1_mean, group2_mean}

5. `mann_whitney_u_test(group1: np.ndarray, group2: np.ndarray, alternative: str = 'two-sided') -> dict`
   - Use scipy.stats.mannwhitneyu (non-parametric alternative to t-test)
   - Handle null values with nan_policy='omit'
   - Return dict: {statistic, p_value, significant (alpha=0.05), effect_size}

6. `kruskal_wallis_test(*groups: np.ndarray) -> dict`
   - Use scipy.stats.kruskal (non-parametric alternative to ANOVA)
   - Test if 3+ groups have different medians
   - Return dict: {statistic, p_value, significant (alpha=0.05), n_groups}

**Dependencies:**
- Import scipy.stats.bootstrap, scipy.stats.mannwhitneyu, scipy.stats.kruskal
- Import numpy as np
- Use pattern from src/analyze.py for statistical utilities

**Key decisions (from RESEARCH.md Pattern 2, Pitfall 4):**
- Use BCa method (bias-corrected and accelerated), not percentile
- Use n_resamples=9999 for good accuracy
- Report standard_error along with CI
- Fall back to percentile or basic method if BCa fails (though unlikely)
- Use Mann-Whitney U for 2-group comparisons (non-parametric)
- Use Kruskal-Wallis for 3+ group comparisons (non-parametric)
  </action>
  <verify>
```bash
# Test bootstrap functions
python3 -c "
import sys
sys.path.insert(0, '.')
from src.bootstrap import bootstrap_mean_ci, mann_whitney_u_test
import numpy as np

# Test bootstrap mean CI
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10] * 10)
result = bootstrap_mean_ci(data, confidence_level=0.95)
print(f'Mean: {result[\"mean\"]:.2f}')
print(f'95% CI: [{result[\"ci_low\"]:.2f}, {result[\"ci_high\"]:.2f}]')
print(f'Standard error: {result[\"standard_error\"]:.4f}')
print(f'Method: {result[\"method\"]}')

# Test Mann-Whitney U test
group1 = np.array([1, 2, 3, 4, 5])
group2 = np.array([4, 5, 6, 7, 8])
result = mann_whitney_u_test(group1, group2)
print(f'Mann-Whitney U: {result[\"statistic\"]:.2f}')
print(f'p-value: {result[\"p_value\"]:.4f}')
print(f'Significant: {result[\"significant\"]}')

print('✓ Bootstrap functions working correctly')
"
```
  </verify>
  <done>
- bootstrap_mean_ci() computes mean with BCa CI
- bootstrap_median_ci() computes median with BCa CI
- bootstrap_correlation_ci() computes correlation with CI
- bootstrap_group_difference_ci() computes group differences
- mann_whitney_u_test() performs non-parametric 2-group test
- kruskal_wallis_test() performs non-parametric 3+ group test
- All functions use BCa method with n_resamples=9999
- Standard errors reported alongside CIs
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute statistical tests pipeline</name>
  <files>scripts/10_statistical_tests.py</files>
  <action>
Create scripts/10_statistical_tests.py as executable script:

**Script structure:**
```python
#!/usr/bin/env python3
"""
Statistical Testing Pipeline - Phase 2 Plan 05 (Part 1)

Performs group comparisons with non-parametric tests and bootstrap CIs.
Quantifies uncertainty for all statistical estimates.

Usage:
    PYTHONPATH=. python3 scripts/10_statistical_tests.py
"""

import polars as pl
import numpy as np
from scipy import stats
from src.bootstrap import (
    bootstrap_mean_ci,
    bootstrap_median_ci,
    bootstrap_group_difference_ci,
    mann_whitney_u_test,
    kruskal_wallis_test
)
from src.clustering import assign_region
from pathlib import Path
from datetime import datetime

def main():
    # Load deduplicated dataset
    input_path = "data/processed/ai_models_deduped.parquet"
    report_path = "reports/statistical_tests_2026-01-18.md"

    print(f"Loading: {input_path}")
    df = pl.read_parquet(input_path)
    print(f"Loaded {df.height} models")

    # Filter to models with valid intelligence scores
    df_valid = df.filter(pl.col("Intelligence Index").is_not_null())
    print(f"Models with valid intelligence: {df_valid.height}")

    # Assign regions
    df_with_region = df_valid.with_columns(
        pl.col("Creator").map_elements(assign_region, return_dtype=pl.Utf8).alias("region")
    )

    # Test 1: Regional comparison (US vs China vs Europe) - STAT-04
    print("\n=== TEST 1: REGIONAL COMPARISON (KRUSKAL-WALLIS) ===")
    regional_results = test_regional_differences(df_with_region)

    # Test 2: Pareto-efficient vs dominated models - Mann-Whitney U
    print("\n=== TEST 2: PARETO EFFICIENT VS DOMINATED ===")
    if "is_pareto_multi_objective" in df.columns:
        pareto_results = test_pareto_differences(df_with_region)
    else:
        print("Skipping: Pareto flags not found")
        pareto_results = None

    # Test 3: Cluster differences (if clusters available)
    print("\n=== TEST 3: CLUSTER COMPARISONS ===")
    provider_clusters = pl.read_parquet("data/processed/provider_clusters.parquet")
    cluster_results = test_cluster_differences(df_with_region, provider_clusters)

    # Test 4: Bootstrap CIs for key metrics - STAT-07, STAT-09
    print("\n=== TEST 4: BOOTSTRAP CONFIDENCE INTERVALS ===")
    bootstrap_results = compute_bootstrap_cis(df_valid)

    # Generate report with significant AND null findings - STAT-11
    print("\n=== GENERATING REPORT ===")
    generate_statistical_report(
        regional_results,
        pareto_results,
        cluster_results,
        bootstrap_results,
        report_path
    )
    print(f"Report: {report_path}")

    print("\n✓ Statistical testing complete")

def test_regional_differences(df: pl.DataFrame) -> dict:
    """Compare US vs China vs Europe providers using Kruskal-Wallis test."""
    metrics = ["Intelligence Index", "price_usd", "Speed(median token/s)"]
    results = {}

    for metric in metrics:
        try:
            # Extract groups by region (cast to Float64)
            groups = []
            for region in ["US", "China", "Europe"]:
                region_data = df.filter(pl.col("region") == region)[metric].drop_nulls()
                if region_data.height > 0:
                    groups.append(region_data.cast(pl.Float64).to_numpy())

            if len(groups) >= 2:
                # Kruskal-Wallis test
                kw_result = kruskal_wallis_test(*groups)
                results[metric] = kw_result

                # Pairwise Mann-Whitney U tests (if significant)
                if kw_result["significant"]:
                    print(f"{metric}: Kruskal-Wallis p={kw_result['p_value']:.4f} (significant)")
                    # Perform pairwise comparisons with FDR correction
                    # ... implementation ...
                else:
                    print(f"{metric}: Kruskal-Wallis p={kw_result['p_value']:.4f} (NOT significant)")

        except Exception as e:
            print(f"Error testing {metric}: {e}")

    return results

def test_pareto_differences(df: pl.DataFrame) -> dict:
    """Compare Pareto-efficient vs dominated models."""
    # ... implementation ...
    pass

def test_cluster_differences(df: pl.DataFrame, clusters: pl.DataFrame) -> dict:
    """Compare provider clusters."""
    # ... implementation ...
    pass

def compute_bootstrap_cis(df: pl.DataFrame) -> dict:
    """Compute bootstrap confidence intervals for key metrics."""
    metrics = {
        "Intelligence Index": df["Intelligence Index"].drop_nulls().cast(pl.Float64).to_numpy(),
        "price_usd": df["price_usd"].drop_nulls().to_numpy(),
        "Speed(median token/s)": df["Speed(median token/s)"].drop_nulls().cast(pl.Float64).to_numpy()
    }

    results = {}
    for metric_name, data in metrics.items():
        if len(data) > 0:
            # Bootstrap mean CI
            mean_result = bootstrap_mean_ci(data)
            # Bootstrap median CI
            median_result = bootstrap_median_ci(data)

            results[metric_name] = {
                "mean": mean_result,
                "median": median_result
            }
            print(f"{metric_name}: Mean 95% CI [{mean_result['ci_low']:.2f}, {mean_result['ci_high']:.2f}]")

    return results

def generate_statistical_report(regional, pareto, cluster, bootstrap, output_path):
    """Generate narrative report with significant and null findings."""
    # Implement report generation with:
    # - Methodology explanation (NARR-07 requirement)
    # - Significant findings (p < 0.05)
    # - **Null findings** (p >= 0.05) - CRITICAL for STAT-11
    # - Bootstrap CIs for all estimates (STAT-09)
    # - Effect sizes and interpretations
    # - Multiple testing correction documentation
    # Follow pattern from scripts/05_quality_report.py

    # ... implementation ...

if __name__ == "__main__":
    main()
```

**Implement generate_statistical_report():**
- Section 1: Methodology (why non-parametric, why bootstrap, why FDR) - NARR-07
- Section 2: Regional Comparison Results (US vs China vs Europe)
- Section 3: Pareto-Efficient vs Dominated
- Section 4: Cluster Comparisons
- Section 5: Bootstrap Confidence Intervals (uncertainty quantification) - STAT-09
- Section 6: **Significant Findings** (p_adjusted < 0.05)
- Section 7: **Null Findings** (p_adjusted >= 0.05) - CRITICAL for STAT-11
- Section 8: Limitations and Recommendations

**Key requirement:** Report BOTH significant and null findings to avoid publication bias (STAT-11).

**Follow pattern from:** scripts/08_correlation_analysis.py (statistical testing), scripts/05_quality_report.py (report generation)
  </action>
  <verify>
```bash
# Execute script
PYTHONPATH=. python3 scripts/10_statistical_tests.py

# Verify output
ls -lh reports/statistical_tests_2026-01-18.md

# Verify report contains both significant and null findings
grep -i "significant" reports/statistical_tests_2026-01-18.md
grep -i "null" reports/statistical_tests_2026-01-18.md
grep -i "methodology" reports/statistical_tests_2026-01-18.md
```
  </verify>
  <done>
- Script executes without errors
- Regional comparison completed (Kruskal-Wallis test)
- Pareto-efficient vs dominated comparison completed
- Cluster comparisons completed
- Bootstrap CIs computed for all key metrics
- Mann-Whitney U tests performed for pairwise comparisons
- Report generated with methodology explanation (NARR-07)
- Report includes significant findings (p < 0.05)
- Report includes null findings (p >= 0.05) - STAT-11 satisfied
- Bootstrap CIs reported with uncertainty quantification - STAT-09 satisfied
  </done>
</task>

<task type="auto">
  <name>Task 3: Execute trend predictions pipeline</name>
  <files>scripts/12_trend_predictions.py</files>
  <action>
Create scripts/12_trend_predictions.py as executable script:

**Script structure:**
```python
#!/usr/bin/env python3
"""
Trend Predictions Pipeline - Phase 2 Plan 05 (Part 2)

Generates simple 2027 trend predictions using linear regression.
Includes uncertainty discussion per NARR-09 requirement.

Usage:
    PYTHONPATH=. python3 scripts/12_trend_predictions.py
"""

import polars as pl
import numpy as np
from sklearn.linear_model import LinearRegression
from scipy import stats
from pathlib import Path
from datetime import datetime

def main():
    # Load deduplicated dataset
    input_path = "data/processed/ai_models_deduped.parquet"
    report_path = "reports/trend_predictions_2026-01-18.md"

    print(f"Loading: {input_path}")
    df = pl.read_parquet(input_path)
    print(f"Loaded {df.height} models")

    # Filter to models with valid intelligence scores
    df_valid = df.filter(pl.col("Intelligence Index").is_not_null())
    print(f"Models with valid intelligence: {df_valid.height}")

    # Simple trend extrapolation
    print("\n=== 2027 TREND PREDICTIONS ===")
    predictions = {}

    # Prediction 1: Intelligence distribution shift
    predictions["intelligence"] = predict_intelligence_trend(df_valid)

    # Prediction 2: Price trends by intelligence tier
    predictions["price_by_tier"] = predict_price_trends(df_valid)

    # Prediction 3: Speed improvements
    predictions["speed"] = predict_speed_trends(df_valid)

    # Generate prediction report with uncertainty discussion
    print("\n=== GENERATING PREDICTION REPORT ===")
    generate_prediction_report(predictions, report_path)
    print(f"Report: {report_path}")

    print("\n✓ Trend predictions complete")

def predict_intelligence_trend(df: pl.DataFrame) -> dict:
    """Predict 2027 intelligence distribution using simple extrapolation."""
    # Use current intelligence distribution as baseline
    # Assume 10% improvement in median intelligence (optimistic scenario)
    # ... implementation ...
    pass

def predict_price_trends(df: pl.DataFrame) -> dict:
    """Predict 2027 pricing trends by intelligence tier."""
    # Group by model_tier, compute price trends
    # Assume linear trends continue
    # ... implementation ...
    pass

def predict_speed_trends(df: pl.DataFrame) -> dict:
    """Predict 2027 speed improvements."""
    # Use current speed distribution
    # Assume 15% improvement in median speed (optimistic)
    # ... implementation ...
    pass

def generate_prediction_report(predictions, output_path):
    """Generate 2027 trend prediction report with uncertainty discussion."""
    # Implement report generation with:
    # - Prediction methodology (linear regression, assumptions)
    # - 2027 predictions for key metrics
    # - **Uncertainty discussion** (CRITICAL for NARR-09)
    # - Limitations of extrapolation
    # - Scenario analysis (optimistic, baseline, pessimistic)
    # - Caveats and assumptions
    # Follow pattern from scripts/05_quality_report.py

    # ... implementation ...

if __name__ == "__main__":
    main()
```

**Implement generate_prediction_report():**
- Section 1: Prediction Methodology (linear regression, assumptions)
- Section 2: 2027 Predictions (intelligence, price, speed)
- Section 3: **Uncertainty Discussion** (CRITICAL for NARR-09)
  - Prediction intervals (not just point forecasts)
  - Model fit assessment (R², residuals)
  - Assumptions (linearity, constant variance, no new competitors)
  - Scenario analysis (optimistic, baseline, pessimistic)
- Section 4: Limitations of Extrapolation
  - Cross-sectional data, not time series
  - Trends may not continue linearly
  - New competitors could disrupt market
  - Regulatory changes, technology breakthroughs
- Section 5: Recommendations

**Key requirement (NARR-09):** Comprehensive uncertainty discussion covering prediction intervals, model limitations, assumptions, and alternative scenarios.

**Follow pattern from:** scripts/08_correlation_analysis.py (statistical modeling), scripts/05_quality_report.py (report generation)

**Key decision (from RESEARCH.md Pitfall 7):** Linear extrapolation is risky. Must flag limitations explicitly, provide scenario analysis, and discuss uncertainty comprehensively.
  </action>
  <verify>
```bash
# Execute script
PYTHONPATH=. python3 scripts/12_trend_predictions.py

# Verify output
ls -lh reports/trend_predictions_2026-01-18.md

# Verify uncertainty discussion
grep -i "uncertainty" reports/trend_predictions_2026-01-18.md
grep -i "limitation" reports/trend_predictions_2026-01-18.md
grep -i "assumption" reports/trend_predictions_2026-01-18.md
grep -i "scenario" reports/trend_predictions_2026-01-18.md
```
  </verify>
  <done>
- Script executes without errors
- 2027 predictions generated for intelligence, price, speed
- Prediction methodology documented
- **Uncertainty discussion** included (NARR-09 satisfied)
- Limitations of extrapolation explained
- Scenario analysis provided (optimistic, baseline, pessimistic)
- Assumptions documented
- Recommendations included
  </done>
</task>

</tasks>

<verification>
## Overall Verification Steps

1. **Bootstrap CI Verification:**
   - BCa method used (not percentile)
   - n_resamples=9999 for accuracy
   - Standard errors reported
   - CIs computed for means, medians, correlations, group differences

2. **Statistical Testing Verification:**
   - Non-parametric tests used (Mann-Whitney U, Kruskal-Wallis)
   - Regional comparison completed (US vs China vs Europe)
   - Pareto-efficient vs dominated comparison completed
   - Cluster comparisons completed
   - Multiple testing correction applied (FDR)

3. **Null Findings Verification (STAT-11):**
   - Null findings reported alongside significant results
   - No filtering out of non-significant tests
   - Publication bias avoided

4. **Uncertainty Quantification Verification (STAT-09):**
   - Bootstrap CIs computed for all estimates
   - Standard errors reported
   - Confidence levels documented

5. **Methodology Documentation Verification (NARR-07):**
   - Non-parametric methods explained
   - Bootstrap approach explained
   - FDR correction explained
   - Why these methods chosen (non-normal distributions)

6. **Trend Prediction Verification (STAT-10, NARR-09):**
   - 2027 predictions generated
   - Prediction methodology documented
   - **Uncertainty discussion** comprehensive
   - Limitations of extrapolation explained
   - Scenario analysis provided

7. **Reproducibility Verification:**
   - Random seeds set (random_state=42)
   - All code documented
   - Functions importable by notebook
</verification>

<success_criteria>
- [ ] Bootstrap CIs computed with BCa method (n_resamples=9999)
- [ ] Non-parametric tests performed (Mann-Whitney U, Kruskal-Wallis)
- [ ] Regional comparison completed (US vs China vs Europe)
- [ ] Pareto-efficient vs dominated comparison completed
- [ ] Cluster comparisons completed
- [ ] Multiple testing correction applied (FDR)
- [ ] Null findings reported (STAT-11 satisfied)
- [ ] Bootstrap CIs reported for all estimates (STAT-09 satisfied)
- [ ] Methodology documented (NARR-07 satisfied)
- [ ] 2027 trend predictions generated (STAT-10 satisfied)
- [ ] Uncertainty discussion comprehensive (NARR-09 satisfied)
- [ ] Limitations and scenario analysis included
- [ ] Functions importable by notebook
- [ ] Dependencies on Plans 02-01, 02-02, 02-03 satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/02-statistical-analysis-domain-insights/02-05-SUMMARY.md` with:
- Bootstrap CI summary (key metrics with 95% CIs)
- Statistical test results (significant and null findings)
- 2027 trend predictions with uncertainty bounds
- Methodology explanation
- Limitations and assumptions
- Uncertainty discussion
- Dependencies on Plans 02-01, 02-02, 02-03
</output>
