---
phase: 02-statistical-analysis-domain-insights
plan: 02
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - src/statistics.py
  - scripts/08_correlation_analysis.py
  - data/processed/correlation_analysis.parquet
  - reports/figures/correlation_heatmap.png
  - reports/correlation_analysis_2026-01-18.md
autonomous: true

must_haves:
  truths:
    - "Spearman correlation matrix computed for all numerical variables"
    - "FDR-corrected p-values identify statistically significant correlations"
    - "Null findings reported alongside significant correlations"
    - "Correlation heatmap visualizes relationships with hierarchical clustering"
  artifacts:
    - path: "src/statistics.py"
      provides: "Statistical analysis functions (Spearman, FDR correction)"
      exports: ["compute_spearman_correlation", "apply_fdr_correction", "compute_correlation_matrix"]
      min_lines: 150
    - path: "scripts/08_correlation_analysis.py"
      provides: "Correlation analysis pipeline"
      min_lines: 100
    - path: "data/processed/correlation_analysis.parquet"
      provides: "Correlation matrix with p-values and FDR correction"
    - path: "reports/figures/correlation_heatmap.png"
      provides: "Visualization of correlation matrix with hierarchical clustering"
    - path: "reports/correlation_analysis_2026-01-18.md"
      provides: "Narrative report of correlation findings with interpretation"
  key_links:
    - from: "scripts/08_correlation_analysis.py"
      to: "data/processed/ai_models_deduped.parquet"
      via: "Input deduplicated dataset"
      pattern: "ai_models_deduped"
    - from: "src/statistics.py"
      to: "scipy.stats"
      via: "Spearman correlation and FDR correction"
      pattern: "from scipy import stats|scipy\\.stats"
    - from: "data/processed/correlation_analysis.parquet"
      to: "scripts/09_pareto_frontier.py"
      via: "Correlation results for frontier analysis"
      pattern: "correlation_analysis"
---

<objective>
Compute Spearman correlation matrix with FDR correction to identify significant relationships between Intelligence, Price, Speed, Latency, and Context Window.

Purpose: Discover which model metrics are significantly associated (positive/negative correlations) while controlling for multiple testing. Use non-parametric Spearman correlation due to highly skewed distributions identified in Phase 1.

Output: Correlation matrix with raw and FDR-corrected p-values, correlation heatmap visualization, and narrative report documenting significant and null findings (STAT-01, STAT-08, STAT-09, STAT-11, NARR-07).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-statistical-analysis-domain-insights/02-RESEARCH.md

@data/processed/ai_models_deduped.parquet
@src/analyze.py
@.planning/phases/02-statistical-analysis-domain-insights/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement statistical analysis utilities</name>
  <files>src/statistics.py</files>
  <action>
Create src/statistics.py with script-as-module pattern:

**Functions to implement:**

1. `compute_spearman_correlation(x: np.ndarray, y: np.ndarray) -> tuple[float, float]`
   - Use scipy.stats.spearmanr(x, y, nan_policy='omit')
   - Return (correlation_coefficient, p_value)
   - Handle null values pairwise (omit NaNs)
   - Document: Spearman is rank-based, appropriate for non-normal distributions

2. `compute_correlation_matrix(df: pl.DataFrame, columns: list[str]) -> tuple[pl.DataFrame, pl.DataFrame]`
   - Compute Spearman correlation for all column pairs
   - Return (correlation_matrix_df, p_value_matrix_df)
   - Use nested loops over column pairs (i <= j for symmetry)
   - Handle nulls with drop_nulls().to_numpy() for each pair
   - Build matrices as numpy arrays first, then convert to Polars DataFrames
   - Column names as row headers and column headers

3. `apply_fdr_correction(p_values: np.ndarray, method: str = 'bh') -> np.ndarray`
   - Use scipy.stats.false_discovery_control(p_values, method=method)
   - Return adjusted p-values
   - Support 'bh' (Benjamini-Hochberg) and 'by' (Benjamini-Yekutieli)
   - Document: FDR controls false discovery rate, more powerful than Bonferroni

4. `interpret_correlation(corr: float, p_value: float, p_adjusted: float, alpha: float = 0.05) -> dict`
   - Classify correlation strength: very weak (0-0.19), weak (0.2-0.39), moderate (0.4-0.59), strong (0.6-0.79), very strong (0.8-1.0)
   - Direction: positive (corr > 0), negative (corr < 0)
   - Significance: significant if p_adjusted < alpha
   - Return dict with interpretation strings

**Dependencies:**
- Import scipy.stats as stats (already in pyproject.toml)
- Import numpy as np
- Import polars as pl
- Use pattern from src/analyze.py for numpy conversion with explicit Float64 casting

**Key decisions (from RESEARCH.md):**
- Use Spearman, not Pearson (all variables are right-skewed per Phase 1)
- Apply FDR (Benjamini-Hochberg) correction for multiple testing
- Report both raw and adjusted p-values
- Interpret correlation strength using standard thresholds
  </action>
  <verify>
```bash
# Test statistical functions
python3 -c "
import sys
sys.path.insert(0, '.')
from src.statistics import compute_spearman_correlation, apply_fdr_correction
import numpy as np

# Test Spearman correlation
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 6, 8, 10])
corr, p = compute_spearman_correlation(x, y)
print(f'Correlation: {corr:.3f}, p-value: {p:.4f}')

# Test FDR correction
p_values = np.array([0.001, 0.03, 0.04, 0.12, 0.57])
adjusted = apply_fdr_correction(p_values)
print(f'Raw p-values: {p_values}')
print(f'Adjusted: {adjusted}')

print('✓ Functions working correctly')
"
```
  </verify>
  <done>
- compute_spearman_correlation() returns correct correlation and p-value
- apply_fdr_correction() adjusts p-values using Benjamini-Hochberg
- Functions handle null values appropriately
- Docstrings document non-parametric approach and FDR correction
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute correlation analysis pipeline</name>
  <files>scripts/08_correlation_analysis.py</files>
  <action>
Create scripts/08_correlation_analysis.py as executable script:

**Script structure:**
```python
#!/usr/bin/env python3
"""
Correlation Analysis Pipeline - Phase 2 Plan 02

Computes Spearman correlation matrix with FDR correction for all numerical variables.
Identifies significant relationships while controlling for multiple testing.

Usage:
    PYTHONPATH=. python3 scripts/08_correlation_analysis.py
"""

import polars as pl
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from src.statistics import compute_spearman_correlation, compute_correlation_matrix, apply_fdr_correction, interpret_correlation
from pathlib import Path
from datetime import datetime

def main():
    # Load deduplicated dataset
    input_path = "data/processed/ai_models_deduped.parquet"
    output_path = "data/processed/correlation_analysis.parquet"
    heatmap_path = "reports/figures/correlation_heatmap.png"
    report_path = "reports/correlation_analysis_2026-01-18.md"

    print(f"Loading: {input_path}")
    df = pl.read_parquet(input_path)
    print(f"Loaded {df.height} models")

    # Define numerical columns for correlation
    numerical_cols = [
        "Intelligence Index",
        "price_usd",
        "Speed(median token/s)",
        "Latency (First Answer Chunk /s)",
        "Context Window"
    ]

    # Filter to models with valid intelligence scores
    df_valid = df.filter(pl.col("Intelligence Index").is_not_null())
    print(f"Models with valid intelligence: {df_valid.height}")

    # Compute correlation matrix
    print("\n=== COMPUTING SPEARMAN CORRELATION ===")
    corr_df, p_df = compute_correlation_matrix(df_valid, numerical_cols)
    print(f"Computed {len(numerical_cols)}x{len(numerical_cols)} correlation matrix")

    # Apply FDR correction to p-values
    print("\n=== APPLYING FDR CORRECTION ===")
    p_values_flat = p_df.select(pl.exclude("column")).to_numpy().flatten()
    p_adjusted_flat = apply_fdr_correction(p_values_flat, method='bh')

    # Reshape adjusted p-values back to matrix
    p_adjusted_matrix = p_adjusted_flat.reshape(len(numerical_cols), len(numerical_cols))
    p_adjusted_df = pl.DataFrame(p_adjusted_matrix, schema=numerical_cols)
    p_adjusted_df = p_adjusted_df.insert_column(0, pl.Series("column", numerical_cols))

    # Merge results
    results = {
        "correlation": corr_df,
        "p_value_raw": p_df,
        "p_value_adjusted": p_adjusted_df
    }

    # Save results
    print(f"\nSaving: {output_path}")
    # Save as separate tables or combined format
    corr_df.write_parquet(output_path.replace(".parquet", "_correlation.parquet"))
    p_df.write_parquet(output_path.replace(".parquet", "_p_raw.parquet"))
    p_adjusted_df.write_parquet(output_path.replace(".parquet", "_p_adjusted.parquet"))

    # Create correlation heatmap
    print(f"\nGenerating heatmap: {heatmap_path}")
    create_correlation_heatmap(corr_df, p_adjusted_df, numerical_cols, heatmap_path)

    # Generate correlation report
    generate_correlation_report(corr_df, p_df, p_adjusted_df, numerical_cols, report_path)
    print(f"Report: {report_path}")

    print("\n✓ Correlation analysis complete")

def create_correlation_heatmap(corr_df: pl.DataFrame, p_adj_df: pl.DataFrame, columns: list[str], output_path: str):
    """Create correlation heatmap with hierarchical clustering and significance annotations."""
    # Extract correlation matrix
    corr_matrix = corr_df.select(pl.exclude("column")).to_numpy()

    # Create figure
    fig, ax = plt.subplots(figsize=(12, 10))

    # Hierarchical clustering
    sns.clustermap(
        corr_matrix,
        cmap='RdBu_r',
        center=0,
        vmin=-1, vmax=1,
        annot=True,
        fmt='.2f',
        xticklabels=columns,
        yticklabels=columns,
        figsize=(12, 10),
        cbar_kws={'label': 'Spearman Correlation'}
    )

    plt.suptitle('Spearman Correlation Matrix with FDR Correction', y=1.02)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def generate_correlation_report(corr_df, p_df, p_adj_df, columns, output_path):
    """Generate narrative report of correlation findings."""
    # Implement report generation with:
    # - Summary table of all correlations
    # - Significant findings (p_adjusted < 0.05)
    # - Null findings (p_adjusted >= 0.05) per STAT-11
    # - Interpretation of correlation strength and direction
    # - Methodology explanation per NARR-07
    # Follow pattern from scripts/05_quality_report.py

    # ... implementation ...

if __name__ == "__main__":
    main()
```

**Implement generate_correlation_report():**
- Summary table with: Variable 1, Variable 2, Correlation, Raw p-value, Adjusted p-value, Significant?, Interpretation
- Significant findings section (p_adjusted < 0.05)
- **Null findings section** (p_adjusted >= 0.05) - CRITICAL for STAT-11
- Methodology explanation (Spearman why, FDR correction why)
- Interpretation of results (what does positive/negative correlation mean?)

**Follow pattern from:** scripts/05_quality_report.py (report structure), src/analyze.py (visualization)
  </action>
  <verify>
```bash
# Execute script
PYTHONPATH=. python3 scripts/08_correlation_analysis.py

# Verify output files
ls -lh data/processed/correlation_analysis_*.parquet
ls -lh reports/figures/correlation_heatmap.png
ls -lh reports/correlation_analysis_2026-01-18.md

# Verify correlation results
python3 -c "
import polars as pl
corr = pl.read_parquet('data/processed/correlation_analysis_correlation.parquet')
print(f'Correlation matrix shape: {corr.shape}')
print(f'Columns: {corr.columns}')
print('Sample correlations:')
print(corr.head())
"
```
  </verify>
  <done>
- Script executes without errors
- Correlation matrix computed (5x5 for 5 numerical variables)
- FDR-corrected p-values computed
- Correlation heatmap generated with hierarchical clustering
- Report generated with significant AND null findings
- Methodology documented (Spearman, FDR correction)
- Results saved to parquet files
  </done>
</task>

</tasks>

<verification>
## Overall Verification Steps

1. **Statistical Method Verification:**
   - Spearman correlation used (not Pearson)
   - FDR (Benjamini-Hochberg) correction applied
   - Nulls handled with pairwise deletion

2. **Correlation Matrix Verification:**
   - Matrix is 5x5 (5 numerical variables)
   - Values in range [-1, 1]
   - Diagonal values = 1.0 (self-correlation)
   - Matrix is symmetric

3. **Significance Testing Verification:**
   - Raw p-values computed
   - FDR-adjusted p-values computed
   - Significance threshold: p_adjusted < 0.05
   - Both significant and null findings reported

4. **Visualization Verification:**
   - Heatmap generated with hierarchical clustering
   - Color scale: red-blue (RdBu_r) centered at 0
   - Annotations show correlation coefficients
   - High resolution (300 DPI)

5. **Documentation Verification:**
   - Methodology explained (why Spearman, why FDR)
   - Significant findings documented
   - Null findings documented (STAT-11 requirement)
   - Interpretation provided for each correlation

6. **Reproducibility Verification:**
   - Random seed set for reproducibility (if applicable)
   - All code documented with comments
   - Functions importable by notebook (script-as-module pattern)
</verification>

<success_criteria>
- [ ] Spearman correlation matrix computed (5x5)
- [ ] FDR-corrected p-values computed using Benjamini-Hochberg
- [ ] Significant correlations identified (p_adjusted < 0.05)
- [ ] Null correlations reported (p_adjusted >= 0.05)
- [ ] Correlation heatmap generated with hierarchical clustering
- [ ] Narrative report with methodology explanation
- [ ] Both significant and null findings documented
- [ ] Results saved to parquet files
- [ ] Functions importable by notebook (script-as-module)
- [ ] Non-parametric methods used throughout (Spearman)
</success_criteria>

<output>
After completion, create `.planning/phases/02-statistical-analysis-domain-insights/02-02-SUMMARY.md` with:
- Correlation matrix summary (which correlations are significant)
- Effect sizes and directions
- Null findings (what's NOT correlated)
- Methodology explanation
- Figures generated
- Dependencies on Plan 02-01 (deduplicated dataset)
</output>
