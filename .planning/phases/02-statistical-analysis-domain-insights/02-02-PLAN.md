---
phase: 02-statistical-analysis-domain-insights
plan: 02
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - src/statistics.py
  - scripts/08_correlation_analysis.py
  - data/processed/correlation_analysis.parquet
  - reports/figures/correlation_heatmap.png
  - reports/figures/context_window_by_intelligence_tier.png
  - reports/correlation_analysis_2026-01-18.md
autonomous: true

must_haves:
  truths:
    - "Spearman correlation matrix computed for all numerical variables"
    - "FDR-corrected p-values identify statistically significant correlations"
    - "Null findings reported alongside significant correlations"
    - "Correlation heatmap visualizes relationships with hierarchical clustering"
    - "Context window distribution analyzed by intelligence tier (STAT-05)"
  artifacts:
    - path: "src/statistics.py"
      provides: "Statistical analysis functions (Spearman, FDR correction)"
      exports: ["compute_spearman_correlation", "apply_fdr_correction", "compute_correlation_matrix", "group_by_quartile"]
      min_lines: 150
    - path: "scripts/08_correlation_analysis.py"
      provides: "Correlation analysis pipeline"
      min_lines: 100
    - path: "data/processed/correlation_analysis.parquet"
      provides: "Correlation matrix with p-values and FDR correction"
    - path: "reports/figures/correlation_heatmap.png"
      provides: "Visualization of correlation matrix with hierarchical clustering"
    - path: "reports/figures/context_window_by_intelligence_tier.png"
      provides: "Context window distribution by intelligence tier (STAT-05)"
    - path: "reports/correlation_analysis_2026-01-18.md"
      provides: "Narrative report of correlation findings with interpretation"
  key_links:
    - from: "scripts/08_correlation_analysis.py"
      to: "data/processed/ai_models_deduped.parquet"
      via: "Input deduplicated dataset"
      pattern: "ai_models_deduped"
    - from: "src/statistics.py"
      to: "scipy.stats"
      via: "Spearman correlation and FDR correction"
      pattern: "from scipy import stats|scipy\\.stats"
    - from: "data/processed/correlation_analysis.parquet"
      to: "scripts/09_pareto_frontier.py"
      via: "Correlation results for frontier analysis"
      pattern: "correlation_analysis"
    - from: "scripts/08_correlation_analysis.py"
      to: "reports/figures/context_window_by_intelligence_tier.png"
      via: "Group-by analysis showing context window by intelligence quartile"
      pattern: "intelligence_quartile|group_by.*intelligence"
---

<objective>
Compute Spearman correlation matrix with FDR correction to identify significant relationships between Intelligence, Price, Speed, Latency, and Context Window. Analyze context window distribution by intelligence tier to reveal how context capacity scales with intelligence.

Purpose: Discover which model metrics are significantly associated (positive/negative correlations) while controlling for multiple testing. Use non-parametric Spearman correlation due to highly skewed distributions identified in Phase 1. Analyze how context window varies across intelligence tiers to answer STAT-05 requirement.

Output: Correlation matrix with raw and FDR-corrected p-values, correlation heatmap visualization, context window distribution by intelligence tier analysis, and narrative report documenting significant and null findings (STAT-01, STAT-05, STAT-08, STAT-09, STAT-11, NARR-07).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-statistical-analysis-domain-insights/02-RESEARCH.md

@data/processed/ai_models_deduped.parquet
@src/analyze.py
@.planning/phases/02-statistical-analysis-domain-insights/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement statistical analysis utilities</name>
  <files>src/statistics.py</files>
  <action>
Create src/statistics.py with script-as-module pattern:

**Functions to implement:**

1. `compute_spearman_correlation(x: np.ndarray, y: np.ndarray) -> tuple[float, float]`
   - Use scipy.stats.spearmanr(x, y, nan_policy='omit')
   - Return (correlation_coefficient, p_value)
   - Handle null values pairwise (omit NaNs)
   - Document: Spearman is rank-based, appropriate for non-normal distributions

2. `compute_correlation_matrix(df: pl.DataFrame, columns: list[str]) -> tuple[pl.DataFrame, pl.DataFrame]`
   - Compute Spearman correlation for all column pairs
   - Return (correlation_matrix_df, p_value_matrix_df)
   - Use nested loops over column pairs (i <= j for symmetry)
   - Handle nulls with drop_nulls().to_numpy() for each pair
   - Build matrices as numpy arrays first, then convert to Polars DataFrames
   - Column names as row headers and column headers

3. `apply_fdr_correction(p_values: np.ndarray, method: str = 'bh') -> np.ndarray`
   - Use scipy.stats.false_discovery_control(p_values, method=method)
   - Return adjusted p-values
   - Support 'bh' (Benjamini-Hochberg) and 'by' (Benjamini-Yekutieli)
   - Document: FDR controls false discovery rate, more powerful than Bonferroni

4. `interpret_correlation(corr: float, p_value: float, p_adjusted: float, alpha: float = 0.05) -> dict`
   - Classify correlation strength: very weak (0-0.19), weak (0.2-0.39), moderate (0.4-0.59), strong (0.6-0.79), very strong (0.8-1.0)
   - Direction: positive (corr > 0), negative (corr < 0)
   - Significance: significant if p_adjusted < alpha
   - Return dict with interpretation strings

5. `group_by_quartile(df: pl.DataFrame, value_col: str, group_col: str = None) -> pl.DataFrame`
   - Create quartile bins for specified value column (e.g., Intelligence Index)
   - Use pl.col(value_col).qcut([0.25, 0.5, 0.75]) to create quartile labels
   - Return DataFrame with quartile column added
   - If group_col provided, also group by that column and compute aggregations
   - Used for STAT-05: group models by intelligence quartile, analyze context window distribution

**Dependencies:**
- Import scipy.stats as stats (already in pyproject.toml)
- Import numpy as np
- Import polars as pl
- Use pattern from src/analyze.py for numpy conversion with explicit Float64 casting

**Key decisions (from RESEARCH.md):**
- Use Spearman, not Pearson (all variables are right-skewed per Phase 1)
- Apply FDR (Benjamini-Hochberg) correction for multiple testing
- Report both raw and adjusted p-values
- Interpret correlation strength using standard thresholds
- For STAT-05: Use quartiles to create intelligence tiers (Q1=low, Q2=mid-low, Q3=mid-high, Q4=high)
  </action>
  <verify>
```bash
# Test statistical functions
python3 -c "
import sys
sys.path.insert(0, '.')
from src.statistics import compute_spearman_correlation, apply_fdr_correction, group_by_quartile
import numpy as np
import polars as pl

# Test Spearman correlation
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 6, 8, 10])
corr, p = compute_spearman_correlation(x, y)
print(f'Correlation: {corr:.3f}, p-value: {p:.4f}')

# Test FDR correction
p_values = np.array([0.001, 0.03, 0.04, 0.12, 0.57])
adjusted = apply_fdr_correction(p_values)
print(f'Raw p-values: {p_values}')
print(f'Adjusted: {adjusted}')

# Test quartile grouping
test_df = pl.DataFrame({
    'intelligence': [10, 20, 30, 40, 50, 60, 70, 80],
    'context_window': [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]
})
grouped = group_by_quartile(test_df, 'intelligence')
print(f'Quartiles: {sorted(grouped[\"intelligence_quartile\"].unique())}')

print('✓ Functions working correctly')
"
```
  </verify>
  <done>
- compute_spearman_correlation() returns correct correlation and p-value
- apply_fdr_correction() adjusts p-values using Benjamini-Hochberg
- group_by_quartile() creates quartile bins for intelligence tiers
- Functions handle null values appropriately
- Docstrings document non-parametric approach and FDR correction
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute correlation analysis pipeline</name>
  <files>scripts/08_correlation_analysis.py</files>
  <action>
Create scripts/08_correlation_analysis.py as executable script:

**Script structure:**
```python
#!/usr/bin/env python3
"""
Correlation Analysis Pipeline - Phase 2 Plan 02

Computes Spearman correlation matrix with FDR correction for all numerical variables.
Identifies significant relationships while controlling for multiple testing.
Analyzes context window distribution by intelligence tier (STAT-05).

Usage:
    PYTHONPATH=. python3 scripts/08_correlation_analysis.py
"""

import polars as pl
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from src.statistics import compute_spearman_correlation, compute_correlation_matrix, apply_fdr_correction, interpret_correlation, group_by_quartile
from pathlib import Path
from datetime import datetime

def main():
    # Load deduplicated dataset
    input_path = "data/processed/ai_models_deduped.parquet"
    output_path = "data/processed/correlation_analysis.parquet"
    heatmap_path = "reports/figures/correlation_heatmap.png"
    tier_plot_path = "reports/figures/context_window_by_intelligence_tier.png"
    report_path = "reports/correlation_analysis_2026-01-18.md"

    print(f"Loading: {input_path}")
    df = pl.read_parquet(input_path)
    print(f"Loaded {df.height} models")

    # Define numerical columns for correlation
    numerical_cols = [
        "Intelligence Index",
        "price_usd",
        "Speed(median token/s)",
        "Latency (First Answer Chunk /s)",
        "Context Window"
    ]

    # Filter to models with valid intelligence scores
    df_valid = df.filter(pl.col("Intelligence Index").is_not_null())
    print(f"Models with valid intelligence: {df_valid.height}")

    # Compute correlation matrix
    print("\n=== COMPUTING SPEARMAN CORRELATION ===")
    corr_df, p_df = compute_correlation_matrix(df_valid, numerical_cols)
    print(f"Computed {len(numerical_cols)}x{len(numerical_cols)} correlation matrix")

    # Apply FDR correction to p-values
    print("\n=== APPLYING FDR CORRECTION ===")
    p_values_flat = p_df.select(pl.exclude("column")).to_numpy().flatten()
    p_adjusted_flat = apply_fdr_correction(p_values_flat, method='bh')

    # Reshape adjusted p-values back to matrix
    p_adjusted_matrix = p_adjusted_flat.reshape(len(numerical_cols), len(numerical_cols))
    p_adjusted_df = pl.DataFrame(p_adjusted_matrix, schema=numerical_cols)
    p_adjusted_df = p_adjusted_df.insert_column(0, pl.Series("column", numerical_cols))

    # Merge results
    results = {
        "correlation": corr_df,
        "p_value_raw": p_df,
        "p_value_adjusted": p_adjusted_df
    }

    # Save results
    print(f"\nSaving: {output_path}")
    # Save as separate tables or combined format
    corr_df.write_parquet(output_path.replace(".parquet", "_correlation.parquet"))
    p_df.write_parquet(output_path.replace(".parquet", "_p_raw.parquet"))
    p_adjusted_df.write_parquet(output_path.replace(".parquet", "_p_adjusted.parquet"))

    # Create correlation heatmap
    print(f"\nGenerating heatmap: {heatmap_path}")
    create_correlation_heatmap(corr_df, p_adjusted_df, numerical_cols, heatmap_path)

    # Analyze context window by intelligence tier (STAT-05)
    print("\n=== CONTEXT WINDOW BY INTELLIGENCE TIER (STAT-05) ===")
    tier_analysis = analyze_context_window_by_tier(df_valid, tier_plot_path)
    print(f"Tier analysis complete: {tier_plot_path}")

    # Generate correlation report
    generate_correlation_report(corr_df, p_df, p_adjusted_df, numerical_cols, tier_analysis, report_path)
    print(f"Report: {report_path}")

    print("\n✓ Correlation analysis complete")

def create_correlation_heatmap(corr_df: pl.DataFrame, p_adj_df: pl.DataFrame, columns: list[str], output_path: str):
    """Create correlation heatmap with hierarchical clustering and significance annotations."""
    # Extract correlation matrix
    corr_matrix = corr_df.select(pl.exclude("column")).to_numpy()

    # Create figure
    fig, ax = plt.subplots(figsize=(12, 10))

    # Hierarchical clustering
    sns.clustermap(
        corr_matrix,
        cmap='RdBu_r',
        center=0,
        vmin=-1, vmax=1,
        annot=True,
        fmt='.2f',
        xticklabels=columns,
        yticklabels=columns,
        figsize=(12, 10),
        cbar_kws={'label': 'Spearman Correlation'}
    )

    plt.suptitle('Spearman Correlation Matrix with FDR Correction', y=1.02)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def analyze_context_window_by_tier(df: pl.DataFrame, output_path: str) -> dict:
    """Analyze context window distribution by intelligence quartile (STAT-05)."""
    # Add intelligence quartile column
    df_with_tiers = group_by_quartile(df, "Intelligence Index")

    # Compute context window statistics by quartile
    tier_stats = df_with_tiers.group_by("intelligence_quartile").agg([
        pl.col("Context Window").count().alias("count"),
        pl.col("Context Window").mean().alias("mean_context_window"),
        pl.col("Context Window").median().alias("median_context_window"),
        pl.col("Context Window").std().alias("std_context_window"),
        pl.col("Context Window").min().alias("min_context_window"),
        pl.col("Context Window").max().alias("max_context_window"),
        pl.col("Intelligence Index").mean().alias("mean_intelligence")
    ]).sort("intelligence_quartile")

    print("\nContext Window by Intelligence Quartile:")
    print(tier_stats)

    # Create visualization
    create_tier_visualization(df_with_tiers, output_path)

    # Return statistics for report
    return {
        "tier_stats": tier_stats,
        "correlation_with_intelligence": df.select(
            pl.corr("Intelligence Index", "Context Window", method="spearman")
        ).item()
    }

def create_tier_visualization(df: pl.DataFrame, output_path: str):
    """Create box plot showing context window distribution by intelligence quartile."""
    plt.figure(figsize=(10, 6))

    # Order quartiles properly
    quartile_order = sorted(df["intelligence_quartile"].unique())

    # Create box plot
    sns.boxplot(
        data=df.to_pandas(),
        x="intelligence_quartile",
        y="Context Window",
        order=quartile_order,
        palette="viridis"
    )

    plt.xlabel("Intelligence Quartile")
    plt.ylabel("Context Window (tokens)")
    plt.title("Context Window Distribution by Intelligence Tier (STAT-05)")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def generate_correlation_report(corr_df, p_df, p_adj_df, columns, tier_analysis, output_path):
    """Generate narrative report of correlation findings."""
    # Implement report generation with:
    # - Summary table of all correlations
    # - Significant findings (p_adjusted < 0.05)
    # - Null findings (p_adjusted >= 0.05) per STAT-11
    # - Interpretation of correlation strength and direction
    # - Methodology explanation per NARR-07
    # - Context window by intelligence tier analysis (STAT-05)
    # Follow pattern from scripts/05_quality_report.py

    # ... implementation ...

if __name__ == "__main__":
    main()
```

**Implement generate_correlation_report():**
- Summary table with: Variable 1, Variable 2, Correlation, Raw p-value, Adjusted p-value, Significant?, Interpretation
- Significant findings section (p_adjusted < 0.05)
- **Null findings section** (p_adjusted >= 0.05) - CRITICAL for STAT-11
- Methodology explanation (Spearman why, FDR correction why)
- Interpretation of results (what does positive/negative correlation mean?)
- **Context Window by Intelligence Tier section** (STAT-05):
  - Table showing mean/median context window for each intelligence quartile
  - Interpretation: How does context capacity scale with intelligence?
  - Visualization reference (box plot figure)

**Follow pattern from:** scripts/05_quality_report.py (report structure), src/analyze.py (visualization)
  </action>
  <verify>
```bash
# Execute script
PYTHONPATH=. python3 scripts/08_correlation_analysis.py

# Verify output files
ls -lh data/processed/correlation_analysis_*.parquet
ls -lh reports/figures/correlation_heatmap.png
ls -lh reports/figures/context_window_by_intelligence_tier.png
ls -lh reports/correlation_analysis_2026-01-18.md

# Verify correlation results
python3 -c "
import polars as pl
corr = pl.read_parquet('data/processed/correlation_analysis_correlation.parquet')
print(f'Correlation matrix shape: {corr.shape}')
print(f'Columns: {corr.columns}')
print('Sample correlations:')
print(corr.head())

# Verify tier analysis in report
import re
with open('reports/correlation_analysis_2026-01-18.md', 'r') as f:
    report = f.read()
    if 'Context Window' in report and 'quartile' in report.lower():
        print('✓ STAT-05 context window tier analysis included in report')
    else:
        print('WARNING: STAT-05 analysis may be missing from report')
"
```
  </verify>
  <done>
- Script executes without errors
- Correlation matrix computed (5x5 for 5 numerical variables)
- FDR-corrected p-values computed
- Correlation heatmap generated with hierarchical clustering
- Context window by intelligence tier analysis completed (STAT-05)
- Tier visualization created (box plot)
- Report generated with significant AND null findings
- STAT-05 analysis documented in report with interpretation
- Methodology documented (Spearman, FDR correction)
- Results saved to parquet files
  </done>
</task>

</tasks>

<verification>
## Overall Verification Steps

1. **Statistical Method Verification:**
   - Spearman correlation used (not Pearson)
   - FDR (Benjamini-Hochberg) correction applied
   - Nulls handled with pairwise deletion

2. **Correlation Matrix Verification:**
   - Matrix is 5x5 (5 numerical variables)
   - Values in range [-1, 1]
   - Diagonal values = 1.0 (self-correlation)
   - Matrix is symmetric

3. **Significance Testing Verification:**
   - Raw p-values computed
   - FDR-adjusted p-values computed
   - Significance threshold: p_adjusted < 0.05
   - Both significant and null findings reported

4. **Context Window Tier Analysis Verification (STAT-05):**
   - Intelligence quartiles created (Q1, Q2, Q3, Q4)
   - Context window statistics computed per quartile (mean, median, std)
   - Box plot visualization created showing distribution by tier
   - Report includes interpretation of how context window scales with intelligence
   - Spearman correlation between intelligence and context window reported

5. **Visualization Verification:**
   - Heatmap generated with hierarchical clustering
   - Color scale: red-blue (RdBu_r) centered at 0
   - Annotations show correlation coefficients
   - High resolution (300 DPI)
   - Box plot for context window by intelligence tier generated

6. **Documentation Verification:**
   - Methodology explained (why Spearman, why FDR)
   - Significant findings documented
   - Null findings documented (STAT-11 requirement)
   - Interpretation provided for each correlation
   - **STAT-05 context window tier analysis documented with interpretation**

7. **Reproducibility Verification:**
   - Random seed set for reproducibility (if applicable)
   - All code documented with comments
   - Functions importable by notebook (script-as-module pattern)
</verification>

<success_criteria>
- [ ] Spearman correlation matrix computed (5x5)
- [ ] FDR-corrected p-values computed using Benjamini-Hochberg
- [ ] Significant correlations identified (p_adjusted < 0.05)
- [ ] Null correlations reported (p_adjusted >= 0.05)
- [ ] Correlation heatmap generated with hierarchical clustering
- [ ] Context window by intelligence tier analysis completed (STAT-05)
- [ ] Intelligence quartiles created (Q1, Q2, Q3, Q4)
- [ ] Context window statistics computed per quartile (mean, median, std, min, max)
- [ ] Box plot visualization created for context window by tier
- [ ] Narrative report with methodology explanation
- [ ] Both significant and null findings documented
- [ ] STAT-05 analysis documented in report with interpretation
- [ ] Results saved to parquet files
- [ ] Functions importable by notebook (script-as-module)
- [ ] Non-parametric methods used throughout (Spearman)
</success_criteria>

<output>
After completion, create `.planning/phases/02-statistical-analysis-domain-insights/02-02-SUMMARY.md` with:
- Correlation matrix summary (which correlations are significant)
- Effect sizes and directions
- Null findings (what's NOT correlated)
- Context window by intelligence tier analysis (STAT-05)
- How context window scales with intelligence
- Methodology explanation
- Figures generated
- Dependencies on Plan 02-01 (deduplicated dataset)
</output>
