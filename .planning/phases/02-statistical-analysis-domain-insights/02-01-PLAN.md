---
phase: 02-statistical-analysis-domain-insights
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/07_duplicate_resolution.py
  - data/processed/ai_models_deduped.parquet
  - reports/duplicate_resolution_2026-01-18.md
autonomous: true

must_haves:
  truths:
    - "Duplicate model names are resolved (no remaining duplicates)"
    - "Each model has a unique identifier for group-by operations"
    - "Resolution strategy is documented and validated"
    - "Dataset is ready for statistical analysis without aggregation errors"
  artifacts:
    - path: "scripts/07_duplicate_resolution.py"
      provides: "Duplicate resolution logic and validation"
      exports: ["resolve_duplicate_models", "validate_resolution"]
      min_lines: 100
    - path: "data/processed/ai_models_deduped.parquet"
      provides: "Deduplicated dataset with unique model IDs"
      contains: "model_id column"
    - path: "reports/duplicate_resolution_2026-01-18.md"
      provides: "Resolution documentation with before/after statistics"
  key_links:
    - from: "scripts/07_duplicate_resolution.py"
      to: "data/processed/ai_models_enriched.parquet"
      via: "pl.read_parquet for input"
      pattern: "pl\\.read_parquet.*ai_models_enriched"
    - from: "data/processed/ai_models_deduped.parquet"
      to: "scripts/08_correlation_analysis.py"
      via: "Input for Phase 2 statistical analysis"
      pattern: "ai_models_deduped"
---

<objective>
Resolve 34 duplicate model names (18.1% of dataset) to enable accurate group-by operations and statistical analysis in Phase 2.

Purpose: Duplicate model names cause incorrect aggregations when grouping by provider or model name. This plan implements a robust resolution strategy that preserves data integrity while creating unique identifiers.

Output: Deduplicated dataset (data/processed/ai_models_deduped.parquet) with unique model_id column, resolution documentation, and validation confirming no remaining duplicates.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-statistical-analysis-domain-insights/02-RESEARCH.md

@data/processed/ai_models_enriched.parquet
@src/quality.py
@src/analyze.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create duplicate resolution utilities</name>
  <files>src/deduplicate.py</files>
  <action>
Create src/deduplicate.py with script-as-module pattern:

**Functions to implement:**

1. `detect_duplicates(df: pl.DataFrame) -> pl.DataFrame`
   - Find all duplicate model names using df.group_by("Model").agg()
   - Return DataFrame with Model, count, and example variations
   - Document duplicate patterns (e.g., same name different context windows)

2. `resolve_duplicate_models(df: pl.DataFrame, strategy: str = "context_window") -> pl.DataFrame`
   - Implement disambiguation strategy using Context Window as differentiator
   - Create model_id column: "Model_ContextWindow" (sanitize names: replace spaces/slashes with underscores)
   - Preserve original Model column for reference
   - Add resolution_source column documenting how ID was created
   - Return DataFrame with unique model_id

3. `validate_resolution(df: pl.DataFrame) -> dict`
   - Verify no remaining duplicates: df.group_by("model_id").agg().filter(pl.col("count") > 1)
   - Assert len(duplicates) == 0, raise AssertionError if duplicates remain
   - Return validation dict: {original_duplicates, resolved_count, remaining_duplicates, pass}

**Dependencies:**
- Import polars as pl
- Use pattern from src/quality.py check_consistency() for duplicate detection
- Follow script-as-module pattern from Phase 1 (src/analyze.py, src/clean.py)

**Key decision (from RESEARCH.md Pitfall 1):**
Use context_window as disambiguator since same model names typically have different context windows (e.g., GPT-4 with 128k vs 8k context). Alternative strategies (version suffix, aggregation) are noted but context_window is primary.
  </action>
  <verify>
```bash
# Run validation
python3 -c "
import sys
sys.path.insert(0, '.')
from src.deduplicate import detect_duplicates, resolve_duplicate_models, validate_resolution
import polars as pl

df = pl.read_parquet('data/processed/ai_models_enriched.parquet')
duplicates = detect_duplicates(df)
print(f'Duplicates found: {len(duplicates)}')

df_resolved = resolve_duplicate_models(df, strategy='context_window')
validation = validate_resolution(df_resolved)
print(f'Validation: {validation}')

assert validation['remaining_duplicates'] == 0, 'Duplicates still exist!'
print('✓ Resolution validated')
"
```
  </verify>
  <done>
- detect_duplicates() returns 34 duplicate model names with details
- resolve_duplicate_models() creates unique model_id for all 188 models
- validate_resolution() confirms 0 remaining duplicates
- model_id column exists and is unique (188 unique values)
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute duplicate resolution pipeline</name>
  <files>scripts/07_duplicate_resolution.py</files>
  <action>
Create scripts/07_duplicate_resolution.py as executable script:

**Script structure:**
```python
#!/usr/bin/env python3
"""
Duplicate Resolution Pipeline - Phase 2 Plan 01

Resolves 34 duplicate model names (18.1%) using context window disambiguation.
Creates unique model_id column for accurate group-by operations.

Usage:
    PYTHONPATH=. python3 scripts/07_duplicate_resolution.py
"""

import polars as pl
from src.deduplicate import detect_duplicates, resolve_duplicate_models, validate_resolution
from pathlib import Path
from datetime import datetime

def main():
    # Load enriched dataset
    input_path = "data/processed/ai_models_enriched.parquet"
    output_path = "data/processed/ai_models_deduped.parquet"
    report_path = "reports/duplicate_resolution_2026-01-18.md"

    print(f"Loading: {input_path}")
    df = pl.read_parquet(input_path)
    print(f"Loaded {df.height} models, {df.width} columns")

    # Detect duplicates
    print("\n=== DUPLICATE DETECTION ===")
    duplicates = detect_duplicates(df)
    print(f"Found {len(duplicates)} duplicate model names")

    # Resolve duplicates
    print("\n=== RESOLVING DUPLICATES ===")
    df_resolved = resolve_duplicate_models(df, strategy="context_window")
    print(f"Created model_id column for {df_resolved.height} models")

    # Validate resolution
    print("\n=== VALIDATION ===")
    validation = validate_resolution(df_resolved)
    print(f"Original duplicates: {validation['original_duplicates']}")
    print(f"Resolved count: {validation['resolved_count']}")
    print(f"Remaining duplicates: {validation['remaining_duplicates']}")
    print(f"Validation: {'PASS' if validation['pass'] else 'FAIL'}")

    # Save deduplicated dataset
    print(f"\nSaving: {output_path}")
    df_resolved.write_parquet(output_path)

    # Generate resolution report
    generate_resolution_report(df, df_resolved, duplicates, validation, report_path)
    print(f"Report: {report_path}")

    print("\n✓ Duplicate resolution complete")

if __name__ == "__main__":
    main()
```

**Implement generate_resolution_report() function:**
- Markdown format with sections: Summary, Duplicates Found, Resolution Strategy, Before/After Statistics, Validation Results
- Table of duplicate models with their new model_ids
- Timestamp and metadata

**Follow pattern from:** scripts/05_quality_report.py (report generation structure)
  </action>
  <verify>
```bash
# Execute script
PYTHONPATH=. python3 scripts/07_duplicate_resolution.py

# Verify output files exist
ls -lh data/processed/ai_models_deduped.parquet
ls -lh reports/duplicate_resolution_2026-01-18.md

# Verify deduplication
python3 -c "
import polars as pl
df = pl.read_parquet('data/processed/ai_models_deduped.parquet')
print(f'Models: {df.height}')
print(f'Unique model_ids: {df[\"model_id\"].n_unique()}')
print(f'Columns: {df.columns}')
assert df['model_id'].n_unique() == df.height, 'model_id not unique!'
print('✓ Deduplication verified')
"
```
  </verify>
  <done>
- Script executes without errors
- data/processed/ai_models_deduped.parquet created (188 models, 17 columns with model_id)
- reports/duplicate_resolution_2026-01-18.md created with documentation
- model_id column is unique (188 unique values = 188 rows)
- Original Model column preserved for reference
  </done>
</task>

</tasks>

<verification>
## Overall Verification Steps

1. **Duplicate Detection Verification:**
   - 34 duplicate model names identified in original dataset
   - Duplicate patterns documented (e.g., same model different context windows)

2. **Resolution Validation:**
   - Zero remaining duplicates after resolution
   - model_id column created with 188 unique values
   - Original Model column preserved

3. **Dataset Integrity:**
   - Row count unchanged (188 models before and after)
   - New columns added: model_id, resolution_source
   - All data preserved, no data loss

4. **Documentation:**
   - Resolution report generated with before/after statistics
   - Strategy documented for reproducibility
   - Examples of duplicate resolutions provided

5. **Readiness for Phase 2:**
   - Deduplicated dataset ready for group-by operations
   - No aggregation errors in subsequent statistical analysis
</verification>

<success_criteria>
- [ ] 34 duplicate model names detected and documented
- [ ] Unique model_id column created (188 unique values)
- [ ] Zero remaining duplicates (validation passes)
- [ ] Deduplicated dataset saved to data/processed/ai_models_deduped.parquet
- [ ] Resolution report generated with full documentation
- [ ] No data loss (188 models preserved)
- [ ] Original Model column preserved for reference
- [ ] Dataset ready for statistical analysis with accurate group-by operations
</success_criteria>

<output>
After completion, create `.planning/phases/02-statistical-analysis-domain-insights/02-01-SUMMARY.md` with:
- Resolution strategy summary
- Before/after statistics
- Duplicate examples and how they were resolved
- Validation results
- Files created/modified
- Dependencies on Phase 1 outputs
</output>
