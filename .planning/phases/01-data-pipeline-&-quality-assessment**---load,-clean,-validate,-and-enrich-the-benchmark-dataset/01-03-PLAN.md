---
phase: 01-data-pipeline
plan: 03
type: execute
wave: 3
depends_on: [01-02]
files_modified:
  - data/interim/02_cleaned.parquet
  - src/clean.py
  - scripts/02_clean.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Price column is converted from messy strings ($4.81) to Float64 values"
    - "Intelligence Index is cleaned of non-numeric suffixes"
    - "Missing values are identified and documented with handling strategy"
    - "Cleaned data is checkpointed for downstream analysis"
  artifacts:
    - path: "data/interim/02_cleaned.parquet"
      provides: "Cleaned dataset with proper data types"
      format: "parquet"
    - path: "src/clean.py"
      provides: "Data cleaning utilities"
      exports: ["clean_price_column", "clean_intelligence_index", "handle_missing_values"]
  key_links:
    - from: "scripts/02_clean.py"
      to: "data/interim/01_loaded.parquet"
      via: "load checkpoint"
      pattern: "read_parquet.*01_loaded"
    - from: "scripts/02_clean.py"
      to: "src/clean.py"
      via: "cleaning function calls"
      pattern: "from src\\.clean import"
    - from: "scripts/02_clean.py"
      to: "data/interim/02_cleaned.parquet"
      via: "sink_parquet checkpoint"
      pattern: "sink_parquet.*02_cleaned"
---

<objective>
Clean messy data values and handle missing data to create a pristine dataset for analysis.

Purpose: Transform raw data into analysis-ready format by extracting numeric values from messy strings, handling missing values appropriately, and documenting all cleaning decisions.
Output: Cleaned dataset with proper data types, missing value analysis report, and checkpointed parquet file.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-CONTEXT.md
@.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Implement price column cleaning</name>
  <files>src/clean.py</files>
  <action>
    Create src/clean.py with data cleaning utilities:

    Import polars as pl

    Define `clean_price_column(lf: pl.LazyFrame) -> pl.LazyFrame` function:
    - Extract "Price (Blended USD/1M Tokens)" column
    - Apply transformations in sequence:
      * .str.strip() - remove leading/trailing whitespace
      * .str.replace("$", "") - remove dollar sign
      * .str.replace(" ", "") - remove spaces
      * .str.replace(",", "") - remove commas (if present in large values)
      * .cast(pl.Float64) - convert to numeric
      * .alias("price_usd") - rename to clean column name
    - Return LazyFrame with new price_usd column
    - Handle any conversion errors by flagging rows (do not drop yet)

    Reference RESEARCH.md Pattern 2 "Clean price column" example
    Add detailed comments explaining each transformation step
    Add docstring with input/output examples ("$4.81 " -> 4.81)

    Note: According to CONTEXT.md, use Claude's discretion for handling conversion errors - flag problematic rows but continue processing
  </action>
  <verify>
    `python -c "from src.clean import clean_price_column; print('Function imported')"` confirms function exists
  </verify>
  <done>
    clean_price_column function exists that extracts numeric values from messy price strings and creates Float64 price_usd column
  </done>
</task>

<task type="auto">
  <name>Implement intelligence index cleaning</name>
  <files>src/clean.py</files>
  <action>
    Add to src/clean.py:

    Define `clean_intelligence_index(lf: pl.LazyFrame) -> pl.LazyFrame` function:
    - Extract "Intelligence Index" column (currently Int64)
    - Check for any non-numeric values or suffixes
    - If data is clean (already Int64), pass through with validation
    - If contains strings like "41\nE", extract numeric part using regex:
      * .str.extract(r"^(\d+)") - extract leading digits
      * .cast(pl.Int64) - convert to integer
    - Validate values are in range [0, 100]
    - Flag any values outside this range
    - Return LazyFrame with validated intelligence_index column

    Add comments explaining why this cleaning step exists (handle quoted multi-line values)
    Add docstring with examples of problematic input and cleaned output
  </action>
  <verify>
    `python -c "from src.clean import clean_intelligence_index; print('Function imported')"` confirms function exists
  </verify>
  <done>
    clean_intelligence_index function exists that validates and extracts numeric intelligence scores, flagging values outside valid range
  </done>
</task>

<task type="auto">
  <name>Implement missing value analysis and handling</name>
  <files>src/clean.py</files>
<action>
    Add to src/clean.py:

    Define `analyze_missing_values(df: pl.DataFrame) -> dict` function:
    - For each column, calculate:
      * null_count: number of null values
      * null_percentage: (null_count / total_rows) * 100
    - Return dict with column names as keys and missing stats as values
    - Print summary to console with verbose logging
    - Identify columns with any missing values

    Define `handle_missing_values(lf: pl.LazyFrame, strategy: dict = None) -> pl.LazyFrame` function:
    - Accept strategy dict mapping column names to handling methods:
      * "drop" - remove rows with nulls in this column
      * "forward_fill" - fill with previous value
      * "backward_fill" - fill with next value
      * "mean" - fill with column mean (numeric only)
      * "median" - fill with column median (numeric only)
      * "zero" - fill with 0 (numeric only)
      * None - leave as null
    - Default strategy: Leave all nulls in place (will be analyzed in plan 04)
    - Document which columns have nulls and recommended handling
    - Return LazyFrame with nulls handled according to strategy

    Reference DATA-05 requirement for missing value analysis
    Add comprehensive docstrings explaining missing value strategies

    Note: According to CONTEXT.md, continue with nulls in enrichment columns - do not drop rows with missing enrichment data
  </action>
  <verify>
    `python -c "from src.clean import analyze_missing_values, handle_missing_values; print('Functions imported')"` confirms functions exist
  </verify>
  <done>
    Missing value analysis function identifies all null values, handling function applies strategies, and both document null patterns for reporting
  </done>
</task>

<task type="auto">
  <name>Execute cleaning pipeline and create checkpoint</name>
  <files>scripts/02_clean.py, data/interim/02_cleaned.parquet</files>
  <action>
    Update scripts/02_clean.py to execute full cleaning pipeline:

    Import functions from src.clean and src.utils
    Import polars as pl

    Pipeline steps:
    1. Load checkpoint from data/interim/01_loaded.parquet
    2. Apply clean_price_column() to create price_usd
    3. Apply clean_intelligence_index() to validate intelligence scores
    4. Analyze missing values with analyze_missing_values()
    5. Apply handle_missing_values() with default strategy (leave nulls)
    6. Collect LazyFrame to materialize cleaned data
    7. Validate cleaned data with Pandera schema from src.validate
    8. Save to data/interim/02_cleaned.parquet using sink_parquet
    9. Print cleaning summary:
       * Rows before/after cleaning
       * Price conversion success rate
       * Intelligence index validation results
       * Missing value counts by column
       * Any warnings or errors encountered

    Add verbose logging at each step using src.utils.setup_logging()
    Document all cleaning decisions in comments
    Handle errors gracefully: if conversion fails, flag row but continue

    Reference RESEARCH.md Pattern 2 "LazyFrame Pipeline with Checkpointing"
  </action>
  <verify>
    `python scripts/02_clean.py` runs successfully
    `test -f data/interim/02_cleaned.parquet` confirms checkpoint created
    `python -c "import polars as pl; df = pl.read_parquet('data/interim/02_cleaned.parquet'); print(df.columns)"` shows price_usd column exists
    `python -c "import polars as pl; df = pl.read_parquet('data/interim/02_cleaned.parquet'); print(df['price_usd'].dtype)"` shows Float64
  </verify>
  <done>
    Cleaning pipeline executes successfully, price column is converted to Float64, intelligence index is validated, missing values are analyzed, and cleaned data is checkpointed to parquet
  </done>
</task>

<task type="auto">
  <name>Generate missing value analysis report</name>
  <files>reports/missing_values.md</files>
  <action>
    Create reports/missing_values.md with missing value analysis:

    Include:
    - Summary table with columns:
      * Column name
      * Null count
      * Null percentage
      * Recommended handling strategy
    - Analysis of missing value patterns:
      * Are missing values random or clustered?
      * Do they correlate with specific creators or model types?
      * Which columns are critical vs optional for analysis?
    - Recommended handling strategy for each column
    - Impact assessment: How will missing values affect downstream analysis?
    - Decision log: Which strategy was chosen and why

    Use analyze_missing_values() output to populate table
    Add narrative interpretation of findings
    Include timestamp and generation metadata

    Reference DATA-05 requirement for missing value documentation
  </action>
  <verify>
    `test -f reports/missing_values.md` confirms report exists
    `grep -q "Column name" reports/missing_values.md` confirms table structure
  </verify>
  <done>
    Missing value analysis report exists with complete statistics, pattern analysis, and recommended handling strategies
  </done>
</task>

</tasks>

<verification>
- [ ] src/clean.py exists with all cleaning functions
- [ ] clean_price_column function handles "$4.81 " format correctly
- [ ] clean_intelligence_index function validates range [0, 100]
- [ ] analyze_missing_values function calculates null percentages
- [ ] handle_missing_values function applies configurable strategies
- [ ] scripts/02_clean.py imports from src.clean
- [ ] Running scripts/02_clean.py completes without errors
- [ ] data/interim/02_cleaned.parquet exists with price_usd as Float64
- [ ] Console output shows cleaning summary and missing value analysis
- [ ] reports/missing_values.md exists with complete analysis
</verification>

<success_criteria>
Messy data values are cleaned (price strings to Float64, intelligence scores validated), missing values are analyzed and documented with handling strategy, and pristine data is checkpointed for downstream analysis.
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-03-SUMMARY.md`
</output>
