---
phase: 01-data-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - requirements.txt
  - src/__init__.py
  - src/utils.py
  - scripts/01_load.py
  - scripts/02_clean.py
  - scripts/03_analyze_distributions.py
  - scripts/04_detect_outliers.py
  - scripts/05_quality_report.py
  - scripts/06_enrich_external.py
  - data/raw/.gitkeep
  - data/interim/.gitkeep
  - data/processed/.gitkeep
  - data/quarantine/.gitkeep
  - data/external/.gitkeep
  - reports/figures/.gitkeep
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Project structure follows numbered script pattern (01_load.py, 02_clean.py, etc.)"
    - "All dependencies are installed and version-locked for reproducibility"
    - "Script-as-module pattern enables functions to be imported by notebooks"
    - "Data directory structure supports the full pipeline workflow"
  artifacts:
    - path: "pyproject.toml"
      provides: "Poetry dependency configuration with version locking"
      contains: "[tool.poetry.dependencies]"
    - path: "requirements.txt"
      provides: "Pip fallback for dependency installation"
      contains: "polars>="
    - path: "src/__init__.py"
      provides: "Python package initialization for imports"
      min_lines: 1
    - path: "scripts/01_load.py"
      provides: "Data loading script with schema validation"
      exports: ["load_data"]
    - path: "data/raw/"
      provides: "Immutable input data storage"
      contains: "ai_models_performance.csv"
    - path: "data/interim/"
      provides: "Intermediate checkpoint storage"
      exists: true
    - path: "data/processed/"
      provides: "Final enriched dataset output"
      exists: true
  key_links:
    - from: "pyproject.toml"
      to: "requirements.txt"
      via: "poetry export"
      pattern: "poetry.*export.*requirements.txt"
    - from: "scripts/*.py"
      to: "src/"
      via: "imports from src package"
      pattern: "from src\\. import"
    - from: "scripts/01_load.py"
      to: "data/raw/"
      via: "reads csv input"
      pattern: "scan_csv.*data/raw"
---

<objective>
Establish the project structure, dependency management, and foundational script template for the data pipeline.

Purpose: Create reproducible development environment with version-locked dependencies and proper directory structure for the full data pipeline workflow.
Output: Poetry project with all dependencies installed, directory structure created, and script template with module pattern ready for implementation.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-CONTEXT.md
@.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Initialize Poetry project and dependencies</name>
  <files>pyproject.toml, requirements.txt</files>
  <action>
    Create pyproject.toml with Poetry configuration including:
    - Project metadata (name, description, version)
    - Python version requirement (^3.10)
    - Core dependencies: polars>=1.0.0, pandera[polars]>=0.21.0, scipy>=1.15.0, scikit-learn>=1.6.0
    - Visualization: matplotlib>=3.10.0, seaborn>=0.13.0
    - Web scraping: requests>=2.32.0, beautifulsoup4>=4.12.0
    - Dev dependencies: pytest>=8.0.0

    Run `poetry install` to create virtual environment and install dependencies
    Run `poetry export -f requirements.txt --output requirements.txt` for pip fallback

    DO NOT use: pandas (use polars), great_expectations (too heavy, use pandera)
  </action>
  <verify>
    `poetry check` validates pyproject.toml syntax
    `poetry show` lists all installed packages
    `test -f requirements.txt` confirms requirements.txt exists
  </verify>
  <done>
    Poetry environment is initialized with all dependencies installed and version-locked in pyproject.toml and requirements.txt
  </done>
</task>

<task type="auto">
  <name>Create directory structure and placeholder files</name>
  <files>data/raw/.gitkeep, data/interim/.gitkeep, data/processed/.gitkeep, data/quarantine/.gitkeep, data/external/.gitkeep, reports/figures/.gitkeep</files>
  <action>
    Create the following directory structure following Cookiecutter Data Science patterns:
    - data/raw/ - immutable input data (move ai_models_performance.csv here)
    - data/interim/ - intermediate checkpoints (parquet files)
    - data/processed/ - final enriched dataset
    - data/quarantine/ - invalid/outlier records
    - data/external/ - scraped external data
    - src/ - shared utilities and helper functions
    - scripts/ - numbered execution scripts
    - reports/figures/ - generated visualizations
    - tests/ - unit tests

    Create .gitkeep files in empty directories to track them in git

    Move existing ai_models_performance.csv from root to data/raw/
  </action>
  <verify>
    `ls -R data/` shows all subdirectories exist
    `test -f data/raw/ai_models_performance.csv` confirms dataset is in correct location
    `test -d src && test -d scripts && test -d reports/figures` confirms all directories created
  </verify>
  <done>
    Complete directory structure exists with dataset in data/raw/ and all required directories created with .gitkeep files
  </done>
</task>

<task type="auto">
  <name>Create shared utilities module</name>
  <files>src/__init__.py, src/utils.py</files>
  <action>
    Create src/__init__.py to mark as Python package

    Create src/utils.py with shared helper functions:
    - `setup_logging(verbose=True)` - configures logging with detailed output
    - `save_checkpoint(df, path)` - saves DataFrame to checkpoint with logging
    - `load_checkpoint(path)` - loads checkpoint with error handling
    - `quarantine_records(df, problematic_df, path)` - saves invalid records separately

    Add comprehensive docstrings following NumPy style guide
    Include type hints for all function parameters and returns

    Reference RESEARCH.md patterns for checkpointing and error handling
  </action>
  <verify>
    `python -c "from src.utils import setup_logging; print('OK')"` confirms import works
    `pylint src/utils.py` (if available) passes without errors
  </verify>
  <done>
    src/utils.py exists with logging, checkpointing, and quarantine helper functions that can be imported by all scripts
  </done>
</task>

<task type="auto">
  <name>Create numbered script templates with module pattern</name>
  <files>scripts/01_load.py, scripts/02_clean.py, scripts/03_analyze_distributions.py, scripts/04_detect_outliers.py, scripts/05_quality_report.py, scripts/06_enrich_external.py</files>
  <action>
    Create template scripts following script-as-module pattern from RESEARCH.md:

    **scripts/01_load.py** - Load data with schema validation
    - Import polars and src.utils
    - Define `load_data(path: str) -> pl.LazyFrame` function
    - Add module docstring explaining purpose
    - Add `if __name__ == "__main__"` block for standalone execution
    - Include detailed comments explaining schema choices

    **scripts/02_clean.py** - Clean messy data values
    - Define `clean_price_column(lf) -> pl.LazyFrame`
    - Define `clean_intelligence_index(lf) -> pl.LazyFrame`
    - Template for other cleaning functions
    - Module docstring and comments

    **scripts/03_analyze_distributions.py** - Distribution analysis
    - Define `analyze_distribution(series) -> dict` using scipy.stats
    - Define `plot_distribution(df, column, output_path)` using matplotlib/seaborn
    - Template for distribution analysis functions

    **scripts/04_detect_outliers.py** - Outlier detection
    - Define `detect_outliers_isolation_forest(df, columns)` using sklearn
    - Define `quarantine_outliers(df, output_path)`
    - Template for outlier detection functions

    **scripts/05_quality_report.py** - Quality assessment
    - Define `generate_quality_report(df, output_path)`
    - Template for quality metrics functions

    **scripts/06_enrich_external.py** - External data enrichment
    - Define `scrape_huggingface_models() -> pl.DataFrame` using requests/bs4
    - Define `enrich_with_external_data(df, external_df) -> pl.DataFrame`
    - Template for enrichment functions

    All scripts must be importable (functions defined at module level)
    All scripts must follow the template pattern from RESEARCH.md
  </action>
  <verify>
    `python -c "from scripts.01_load import load_data; print('OK')"` confirms each script is importable
    `python scripts/01_load.py --help 2>&1 | grep -q "usage"` or script runs without import errors
  </verify>
  <done>
    All 6 numbered scripts exist with function templates, module docstrings, and follow script-as-module pattern for importability
  </done>
</task>

</tasks>

<verification>
- [ ] Poetry project initialized with all dependencies installed
- [ ] pyproject.toml contains correct versions (polars>=1.0.0, pandera[polars]>=0.21.0)
- [ ] requirements.txt generated from poetry export
- [ ] All directories created (data/raw, data/interim, data/processed, data/quarantine, data/external, src, scripts, reports/figures)
- [ ] Dataset moved to data/raw/ai_models_performance.csv
- [ ] src/utils.py contains helper functions with type hints and docstrings
- [ ] All 6 numbered scripts exist with module pattern (functions importable)
- [ ] Each script has comprehensive module docstring
- [ ] Running scripts does not cause import errors
</verification>

<success_criteria>
Project structure is fully set up with reproducible dependencies, directory hierarchy matches research recommendations, and all script templates follow the script-as-module pattern enabling both standalone execution and notebook integration.
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-01-SUMMARY.md`
</output>
