---
phase: 01-data-pipeline
plan: 06
type: execute
wave: 4
depends_on: [01-03, 01-04]
files_modified:
  - reports/quality_2026-01-18.md
  - scripts/05_quality_report.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Comprehensive quality report documents all distributions, missing values, outliers, and sanity checks"
    - "Report includes visualizations embedded as markdown links"
    - "Sanity checks validate data quality across all dimensions"
    - "Report is timestamped and saved to reports/ directory"
  artifacts:
    - path: "reports/quality_2026-01-18.md"
      provides: "Comprehensive data quality assessment report"
      format: "markdown"
    - path: "scripts/05_quality_report.py"
      provides: "Quality report generation script"
      exports: ["generate_quality_report", "perform_sanity_checks"]
  key_links:
    - from: "scripts/05_quality_report.py"
      to: "data/interim/03_distributions_analyzed.parquet"
      via: "load analyzed data with statistics"
      pattern: "read_parquet.*03_distributions_analyzed"
    - from: "scripts/05_quality_report.py"
      to: "reports/figures/"
      via: "embed distribution plots"
      pattern: "figures/.*\\.png"
    - from: "scripts/05_quality_report.py"
      to: "reports/quality_*.md"
      via: "save generated report"
      pattern: "quality_.*\\.md"
---

<objective>
Generate a comprehensive data quality assessment report documenting all findings from the pipeline.

Purpose: Create a complete quality assessment that summarizes data distributions, missing values, outliers, sanity checks, and provides actionable insights for downstream analysis, serving as the foundation for understanding the dataset's strengths and limitations.
Output: Timestamped quality report in markdown format with embedded visualizations, statistics, and narrative interpretation.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-CONTEXT.md
@.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Implement sanity check functions</name>
  <files>src/quality.py</files>
  <action>
    Create src/quality.py with data quality assessment utilities:

    Import polars as pl

    Define `check_accuracy(df: pl.DataFrame) -> dict` function:
    - Validate Intelligence Index is in range [0, 100]
    - Validate price_usd >= 0
    - Validate speed >= 0
    - Validate latency >= 0
    - Validate context_window >= 0
    - Count violations for each check
    - Return dict with check results (pass/fail, violation_count, violations)

    Define `check_completeness(df: pl.DataFrame) -> dict` function:
    - Calculate null count and percentage for each column
    - Calculate rows_with_any_null: number of rows with >=1 null value
    - Calculate rows_complete: number of rows with 0 null values
    - Calculate completeness_percentage: (rows_complete / total_rows) * 100
    - Return dict with completeness metrics

    Define `check_consistency(df: pl.DataFrame) -> dict` function:
    - Check for duplicate model names
    - Check context_window values are realistic (0 to 2M tokens)
    - Check creator names are consistent (no slight variations)
    - Check price per intelligence is reasonable (no extreme outliers)
    - Return dict with consistency check results

    Define `check_validity(df: pl.DataFrame) -> dict` function:
    - Validate Creator is in expected set (OpenAI, Anthropic, Google, etc.)
    - Validate data types match schema
    - Validate no impossible combinations (e.g., speed=0 but latency>0)
    - Return dict with validity check results

    Define `perform_sanity_checks(df: pl.DataFrame) -> dict` function:
    - Call all check functions (accuracy, completeness, consistency, validity)
    - Aggregate results into single dict
    - Calculate overall_quality_score: average of all dimension scores
    - Return comprehensive sanity check results

    Reference RESEARCH.md "Pitfall 4: Data Quality Metrics Overwhelm" - focus on 6 dimensions with 2-3 key metrics each
    Add comprehensive docstrings explaining each sanity check and its importance
  </action>
  <verify>
    `python -c "from src.quality import perform_sanity_checks; print('Function imported')"` confirms function exists
  </verify>
  <done>
    Sanity check functions exist that validate accuracy, completeness, consistency, and validity dimensions of data quality
  </done>
</task>

<task type="auto">
  <name>Implement quality report generation</name>
  <files>src/quality.py</files>
  <action>
    Add to src/quality.py:

    Define `generate_quality_report(df: pl.DataFrame, distributions_stats: dict, output_path: str)` function:
    - Get current date for timestamp in filename
    - Create markdown report with sections:

    **Header:**
    - Title: "Data Quality Assessment Report"
    - Date: generation timestamp
    - Dataset: ai_models_enriched.parquet
    - Rows x Columns: shape

    **Executive Summary:**
    - Overall quality score (0-100%)
    - Key findings (3-5 bullet points)
    - Critical issues (if any)
    - Data readiness for analysis

    **Data Dimensions (6 dimensions from RESEARCH.md):**
    1. Accuracy - validation results for ranges and constraints
    2. Completeness - missing value analysis with table
    3. Consistency - duplicate and consistency checks
    4. Validity - schema and enum validation
    5. Integrity - referential integrity (if joins performed)
    6. Timeliness - data freshness note (static dataset)

    **Distribution Analysis:**
    - Summary table for each numerical variable (from distributions_stats)
    - Interpretation of skewness, kurtosis, normality
    - Links to distribution plots: ![Distribution](figures/{column}_distribution.png)

    **Outlier Analysis:**
    - Outlier detection results (method, count, percentage)
    - Outlier examples with model names and values
    - Recommendations for handling outliers

    **Sanity Check Results:**
    - All sanity checks with pass/fail status
    - Violations detailed with examples
    - Action items for addressing violations

    **Data Quality Issues Found:**
    - List of all issues discovered during pipeline
    - Severity rating (critical, warning, info)
    - Recommended actions

    **Next Steps:**
    - Ready for Phase 2: Statistical Analysis
    - Known limitations to consider
    - Recommended preprocessing for specific analyses

    **Metadata:**
    - Generation timestamp
    - Pipeline version
    - Dependencies and versions

    Write report to output_path using Python file I/O
    Return path to generated report

    Reference CONTEXT.md: "Comprehensive - full distribution analysis with statistics for every column"
    Reference CONTEXT.md: "Generate figures and save to reports/figures/"
    Include narrative interpretation based on findings (Claude's discretion)

    Note: According to CONTEXT.md, include Claude's discretion narrative interpretation based on findings
  </action>
  <verify>
    `python -c "from src.quality import generate_quality_report; print('Function imported')"` confirms function exists
  </verify>
  <done>
    generate_quality_report function exists that creates comprehensive markdown report with all sections, embedded figures, statistics tables, and narrative interpretation
  </done>
</task>

<task type="auto">
  <name>Execute quality report generation</name>
  <files>scripts/05_quality_report.py, reports/quality_2026-01-18.md</files>
  <action>
    Update scripts/05_quality_report.py to execute quality assessment:

    Import functions from src.quality, src.analyze, src.clean, src.utils
    Import polars as pl
    Import datetime

    Pipeline steps:
    1. Load analyzed dataset from data/interim/03_distributions_analyzed.parquet
    2. Perform sanity checks using perform_sanity_checks()
    3. Print sanity check summary to console:
       * Overall quality score
       * Dimensions that passed/failed
       * Critical issues found
    4. Load distribution statistics (from plan 04 results or recalculate)
    5. Generate quality report using generate_quality_report():
       * Pass DataFrame, distribution stats, output path
       * Use timestamp: reports/quality_{date}.md
    6. Print "Quality report generated at {path}"
    7. Verify report file exists
    8. Print sections generated:
       * Executive summary
       * Distribution analysis
       * Outlier analysis
       * Sanity checks
       * Data quality issues
       * Next steps

    Use verbose logging throughout
    Add progress indicators for long operations
    Document any issues encountered during report generation

    Note: Use current date (2026-01-18) for report filename as per CONTEXT.md example
  </action>
  <verify>
    `python scripts/05_quality_report.py` runs successfully
    `test -f reports/quality_2026-01-18.md` confirms report exists
    `grep -q "Executive Summary" reports/quality_2026-01-18.md` confirms report structure
    `grep -q "!\[.*\](figures/.*\.png)" reports/quality_2026-01-18.md` confirms figures embedded
  </verify>
  <done>
    Quality report generation executes successfully, all sections are created with complete statistics and visualizations, and report is saved with timestamp to reports/ directory
  </done>
</task>

<task type="auto">
  <name>Create pipeline completion summary</name>
  <files>reports/pipeline_summary.md</files>
  <action>
    Create reports/pipeline_summary.md documenting the complete pipeline:

    Include:
    - **Pipeline Overview:**
      * Phase 1 objective
      * Pipeline stages completed
      * Input: ai_models_performance.csv (188 rows, 7 columns)
      * Output: ai_models_enriched.parquet (188 rows, 10+ columns)

    - **Stages Completed:**
      1. Setup and dependencies (plan 01)
      2. Data loading and validation (plan 02)
      3. Data cleaning (plan 03)
      4. Distribution analysis and outlier detection (plan 04)
      5. External data enrichment (plan 05)
      6. Quality assessment reporting (plan 06)

    - **Artifacts Created:**
      * Checkpoints: data/interim/*.parquet (3 files)
      * External data: data/external/*.parquet (1+ files)
      * Final dataset: data/processed/ai_models_enriched.parquet
      * Visualizations: reports/figures/*.png (5+ files)
      * Reports: data_structure.md, missing_values.md, distributions.md, enrichment_coverage.md, quality_*.md

    - **Data Quality Summary:**
      * Overall quality score
      * Columns cleaned (price, intelligence index)
      * Missing value handling
      * Outlier detection results
      * Enrichment coverage

    - **Known Limitations:**
      * Data quality issues that couldn't be resolved
      * Missing external data
      * Assumptions made during cleaning

    - **Reproducibility:**
      * Dependency versions (pyproject.toml)
      * Random seeds (42 for Isolation Forest)
      * Data sources and provenance

    - **Next Phase:**
      * Ready for Phase 2: Statistical Analysis
      * Key files to use: ai_models_enriched.parquet
      * Recommended starting points

    Reference NARR-06 requirement for code comments explaining analysis choices
    Include timestamp and generation metadata
  </action>
  <verify>
    `test -f reports/pipeline_summary.md` confirms summary exists
    `grep -q "Pipeline Overview" reports/pipeline_summary.md` confirms structure
    `grep -q "Phase 2" reports/pipeline_summary.md` confirms next phase documented
  </verify>
  <done>
    Pipeline completion summary exists documenting all stages, artifacts, data quality findings, limitations, and readiness for Phase 2
  </done>
</task>

</tasks>

<verification>
- [ ] src/quality.py exists with all quality assessment functions
- [ ] perform_sanity_checks function validates all 6 quality dimensions
- [ ] generate_quality_report function creates comprehensive markdown report
- [ ] scripts/05_quality_report.py imports from src.quality
- [ ] Running scripts/05_quality_report.py completes without errors
- [ ] reports/quality_2026-01-18.md exists with all sections
- [ ] Report includes executive summary, distributions, outliers, sanity checks
- [ ] Report embeds visualization links to reports/figures/
- [ ] reports/pipeline_summary.md exists documenting complete pipeline
- [ ] All analysis choices are documented in comments (NARR-06)
</verification>

<success_criteria>
Comprehensive quality report is generated documenting all data distributions, missing values, outliers, sanity checks with embedded visualizations, pipeline completion summary is created, and all analysis choices are documented for reproducibility.
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-06-SUMMARY.md`
</output>
