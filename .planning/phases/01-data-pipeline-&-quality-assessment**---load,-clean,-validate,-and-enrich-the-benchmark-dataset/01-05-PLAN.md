---
phase: 01-data-pipeline
plan: 05
type: execute
wave: 3
depends_on: [01-02]
files_modified:
  - data/external/huggingface_models.parquet
  - data/processed/ai_models_enriched.parquet
  - src/enrich.py
  - scripts/06_enrich_external.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "External data sources are scraped for model release dates and announcements"
    - "Provenance is tracked with source_url, retrieved_at, and retrieved_by columns"
    - "Raw scraped data is saved to data/external/ for reproducibility"
    - "Dataset is enriched with external data via left join (nulls if no match)"
  artifacts:
    - path: "data/external/huggingface_models.parquet"
      provides: "Scraped external data with provenance"
      format: "parquet"
    - path: "data/processed/ai_models_enriched.parquet"
      provides: "Final enriched dataset for analysis"
      format: "parquet"
    - path: "src/enrich.py"
      provides: "External data enrichment utilities"
      exports: ["scrape_huggingface_models", "enrich_with_external_data"]
  key_links:
    - from: "scripts/06_enrich_external.py"
      to: "data/interim/02_cleaned.parquet"
      via: "load cleaned dataset"
      pattern: "read_parquet.*02_cleaned"
    - from: "scripts/06_enrich_external.py"
      to: "src/enrich.py"
      via: "enrichment function calls"
      pattern: "from src\\.enrich import"
    - from: "src/enrich.py"
      to: "https://huggingface.co/open-llm-leaderboard"
      via: "web scraping with requests"
      pattern: "requests\\.get.*huggingface"
---

<objective>
Enrich the dataset with external data sources including model release dates, provider announcements, and market events.

Purpose: Add temporal and contextual metadata to enable time-based analysis and market trend insights, tracking provenance for reproducibility and updateability.
Output: Enriched dataset with external metadata, saved to data/processed/ as the final analysis-ready dataset.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-CONTEXT.md
@.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Implement web scraping utilities</name>
  <files>src/enrich.py</files>
  <action>
    Create src/enrich.py with external data enrichment utilities:

    Import requests, BeautifulSoup, datetime, time, polars as pl

    Define `scrape_huggingface_models() -> pl.DataFrame` function:
    - Set base_url = "https://huggingface.co/open-llm-leaderboard"
    - Initialize empty list models_data = []
    - Try-except block for error handling:
      * Send GET request with User-Agent header to avoid blocking
      * Use response.raise_for_status() to check for HTTP errors
      * Parse HTML with BeautifulSoup
      * Extract model information (selectors depend on actual page structure):
        - Model name from model name element
        - Release date from date element if available
        - Benchmark scores if available
        - Provider/organization if available
      * For each model found, append dict with:
        * model: str
        * release_date: str (or None)
        * benchmark_score: float (or None)
        * provider: str (or None)
        * source_url: str (base_url or specific model page)
        * retrieved_at: datetime.now().isoformat()
        * retrieved_by: "scrape_huggingface_models"
      * Add time.sleep(1) for rate limiting between requests
      * Handle pagination if present (loop through pages)
    - On exception: print error message, return empty pl.DataFrame()
    - Convert models_data list to pl.DataFrame
    - Return DataFrame

    Reference RESEARCH.md "Web Scraping for Model Release Dates" example
    Add comprehensive docstring explaining data source and limitations
    Add comments about rate limiting and respectful scraping practices
    Note that actual HTML selectors will need inspection and adjustment

    Define `scrape_provider_announcements() -> pl.DataFrame` function:
    - Similar structure to scrape_huggingface_models
    - Target provider blogs/news pages (OpenAI, Anthropic, Google, etc.)
    - Extract model announcements with dates
    - Add provenance columns (source_url, retrieved_at, retrieved_by)
    - Return DataFrame

    Note: According to CONTEXT.md, use automated collection where possible but accept best-effort coverage
  </action>
  <verify>
    `python -c "from src.enrich import scrape_huggingface_models; print('Function imported')"` confirms function exists
  </verify>
  <done>
    Web scraping functions exist that can fetch external data from HuggingFace and provider sources with proper rate limiting, error handling, and provenance tracking
  </done>
</task>

<task type="auto">
  <name>Implement data enrichment utilities</name>
  <files>src/enrich.py</files>
  <action>
    Add to src/enrich.py:

    Define `enrich_with_external_data(base_df: pl.DataFrame, external_df: pl.DataFrame, join_key: str = "model") -> pl.DataFrame` function:
    - Validate join_key exists in both DataFrames
    - Perform left join: base_df.join(external_df, on=join_key, how="left")
    - Left join ensures all original models are kept (nulls if no match)
    - Add enrichment metadata columns:
      * enriched_at: datetime.now()
      * enrichment_source: external_df source column or "external_scraping"
      * coverage_rate: (non-null enrichment rows / total rows) calculated as percentage
    - Return enriched DataFrame

    Define `add_derived_columns(df: pl.DataFrame) -> pl.DataFrame` function:
    - Create derived metrics for analysis:
      * price_per_intelligence_point: price_usd / intelligence_index (handle division by zero)
      * speed_intelligence_ratio: speed / intelligence_index
      * model_tier: extract from model name using regex (xhigh, high, medium, low, mini)
      * log_context_window: log10(context_window) for better visualization
      * price_per_1k_tokens: price_usd / 1000 (more intuitive scale)
    - Handle edge cases (null values, division by zero)
    - Return DataFrame with new columns

    Define `calculate_enrichment_coverage(df: pl.DataFrame, enrichment_columns: list[str]) -> dict` function:
    - For each enrichment column, calculate:
      * non_null_count: number of rows with data
      * null_count: number of rows with nulls
      * coverage_percentage: (non_null_count / total_rows) * 100
    - Return dict with coverage statistics
    - Print summary to console

    Add comprehensive docstrings for each function
    Add type hints for all parameters and returns
  </action>
  <verify>
    `python -c "from src.enrich import enrich_with_external_data, add_derived_columns, calculate_enrichment_coverage; print('Functions imported')"` confirms functions exist
  </verify>
  <done>
    Enrichment utilities exist that join external data via left join, add derived analysis columns, and calculate coverage statistics for documentation
  </done>
</task>

<task type="auto">
  <name>Execute external data scraping</name>
  <files>scripts/06_enrich_external.py, data/external/huggingface_models.parquet</files>
  <action>
    Update scripts/06_enrich_external.py to execute web scraping:

    Import functions from src.enrich and src.utils

    Scraping pipeline:
    1. Create data/external/ directory if not exists
    2. Print "Starting external data collection..." to console
    3. Call scrape_huggingface_models():
       * Print progress messages (e.g., "Fetching HuggingFace leaderboard...")
       * Handle errors gracefully
       * Store result in huggingface_df variable
    4. If huggingface_df is not empty:
       * Save to data/external/huggingface_models.parquet
       * Print f"Retrieved {huggingface_df.shape[0]} models from HuggingFace"
    5. Optionally call scrape_provider_announcements():
       * Scrape from multiple provider sources
       * Save each to data/external/{provider}_announcements.parquet
       * Print progress for each source
    6. If all scraping fails (empty results):
       * Print warning: "No external data retrieved. Will proceed with base dataset only."
       * Print "Consider manual data entry for top 20 models."
    7. Combine all external data into single DataFrame if multiple sources
    8. Save combined external data to data/external/all_external_data.parquet

    Use verbose logging to show progress
    Implement rate limiting (1 second delay between requests)
    Handle HTTP errors, timeouts, and parsing errors gracefully
    Document any scraping issues in comments

    Reference CONTEXT.md: "best-effort coverage - document coverage rate and proceed regardless"
  </action>
  <verify>
    `python scripts/06_enrich_external.py` runs successfully (may produce warnings if scraping fails)
    `test -f data/external/huggingface_models.parquet` or `test -f data/external/all_external_data.parquet` confirms external data saved
    `ls data/external/*.parquet | wc -l` shows at least one external data file
  </verify>
  <done>
    External data scraping executes successfully, retrieving model metadata from HuggingFace and/or provider sources, with results saved to data/external/ directory
  </done>
</task>

<task type="auto">
  <name>Execute enrichment pipeline and create final dataset</name>
  <files>data/processed/ai_models_enriched.parquet</files>
  <action>
    Add to scripts/06_enrich_external.py after scraping:

    Enrichment pipeline:
    1. Load cleaned dataset from data/interim/02_cleaned.parquet
    2. Load external data from data/external/ (if exists)
    3. If external data exists:
       * Standardize model names (handle case sensitivity, special characters)
       * Call enrich_with_external_data() with left join on "Model" column
       * Print "Enriched dataset with external data"
    4. If external data doesn't exist or is empty:
       * Print "Proceeding without external enrichment (coverage would be 0%)"
       * Continue with base dataset only
    5. Call add_derived_columns() to create analysis metrics
    6. Call calculate_enrichment_coverage() to document coverage rate
    7. Print enrichment summary:
       * Coverage percentage for each enrichment column
       * Total rows before and after enrichment (should be same - left join)
       * Number of models with release dates, benchmark scores, etc.
    8. Create data/processed/ directory if not exists
    9. Save final enriched dataset to data/processed/ai_models_enriched.parquet
    10. Print final summary:
        * "Final enriched dataset saved to data/processed/ai_models_enriched.parquet"
        * "Shape: {rows} x {columns}"
        * "Ready for Phase 2: Statistical Analysis"

    Use verbose logging throughout
    Document any data quality issues discovered during enrichment
    Handle model name mismatches gracefully (some models won't have external data)

    Reference RESEARCH.md "Pitfall 5: External Data Enrichment Provenance Loss" for proper metadata tracking
  </action>
  <verify>
    `python scripts/06_enrich_external.py` runs successfully
    `test -f data/processed/ai_models_enriched.parquet` confirms final dataset exists
    `python -c "import polars as pl; df = pl.read_parquet('data/processed/ai_models_enriched.parquet'); print(df.shape)"` shows shape (188, >=10) with enrichment columns
    `python -c "import polars as pl; df = pl.read_parquet('data/processed/ai_models_enriched.parquet'); print(df.columns)"` shows derived columns like price_per_intelligence_point
  </verify>
  <done>
    Enrichment pipeline completes successfully, external data is joined with provenance tracking, derived columns are created, coverage is documented, and final enriched dataset is saved to data/processed/
  </done>
</task>

<task type="auto">
  <name>Generate enrichment coverage report</name>
  <files>reports/enrichment_coverage.md</files>
  <action>
    Create reports/enrichment_coverage.md with enrichment analysis:

    Include:
    - Data sources used:
      * HuggingFace Open LLM Leaderboard
      * Provider announcement pages (list which ones)
      * Any other sources
    - Coverage statistics table:
      * Enrichment column
      * Rows with data
      * Rows with nulls
      * Coverage percentage
    - Model name matching analysis:
      * How many models matched external sources
      * Common mismatch patterns (case, spacing, abbreviations)
      * Examples of unmatched models
    - Data quality assessment:
      * Reliability of each external source
      * Known issues or limitations
    - Recommendations:
      * Manual data entry opportunities
      * Additional sources to consider
      * Update schedule (how often to refresh)

    Use calculate_enrichment_coverage() output to populate table
    Add narrative interpretation of findings
    Include timestamp and generation metadata

    Reference DATA-08 requirement for external data enrichment documentation
  </action>
  <verify>
    `test -f reports/enrichment_coverage.md` confirms report exists
    `grep -q "Coverage" reports/enrichment_coverage.md` confirms coverage analysis
  </verify>
  <done>
    Enrichment coverage report exists documenting data sources, match rates, coverage percentages, and recommendations for improvement
  </done>
</task>

</tasks>

<verification>
- [ ] src/enrich.py exists with all enrichment functions
- [ ] scrape_huggingface_models function implements web scraping with rate limiting
- [ ] enrich_with_external_data function performs left join with provenance tracking
- [ ] add_derived_columns function creates analysis metrics
- [ ] scripts/06_enrich_external.py imports from src.enrich
- [ ] Running scripts/06_enrich_external.py completes without errors
- [ ] data/external/ directory contains scraped data parquet files
- [ ] data/processed/ai_models_enriched.parquet exists with enrichment columns
- [ ] Final dataset has derived columns (price_per_intelligence_point, model_tier, etc.)
- [ ] reports/enrichment_coverage.md exists with coverage statistics
</verification>

<success_criteria>
External data is scraped from HuggingFace and provider sources with provenance tracking, dataset is enriched via left join (preserving all models), derived analysis columns are created, coverage is documented, and final enriched dataset is saved for Phase 2 analysis.
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline-&-quality-assessment**---load,-clean,-validate,-and-enrich-the-benchmark-dataset/01-05-SUMMARY.md`
</output>
